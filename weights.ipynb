{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-science-bowl-2019/test.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/train.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    reduce_train['accuracy_group'].hist()\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohenkappa(ypred, y):\n",
    "    y = y.get_label().astype(\"int\")\n",
    "    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n",
    "    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n",
    "    return \"cappa\", loss, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that convert the raw data into processed features\n",
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "        \n",
    "    # last features\n",
    "    sessions_count = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "                    \n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features['installation_session_count'] = sessions_count\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                              ('var_event_id', event_id_count),\n",
    "                               ('var_title', title_count),\n",
    "                               ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                 \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        sessions_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    categoricals = ['session_title']\n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy_group'\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = self.get_params()\n",
    "        self.y_pred, self.score, self.model = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(reduce_train), ))\n",
    "        y_pred = np.zeros((len(reduce_test), ))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]))\n",
    "        _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred)\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        return y_pred, loss_score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 \n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'n_estimators':5000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.75,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'feature_fraction': 0.9,\n",
    "                    'max_depth': 15,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                    'early_stopping_rounds': 100}\n",
    "        return params\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 \n",
    "        return xgb.train(self.params, train_set, \n",
    "                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n",
    "                         verbose_eval=verbosity, early_stopping_rounds=100)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = xgb.DMatrix(x_train, y_train)\n",
    "        val_set = xgb.DMatrix(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "    \n",
    "    def convert_x(self, x):\n",
    "        return xgb.DMatrix(x)\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'colsample_bytree': 0.4, 'lambda_l1': 0.0, 'lambda_l2': 2.6757021823338296, 'learning_rate': 0.05, \n",
    "                  'max_delta_step': 5.0, 'max_depth':8, 'min_split_loss': 5.0, 'subsample': 1.0}\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostRegressor(**self.params)\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity, \n",
    "                cat_features=self.categoricals)\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'loss_function': 'RMSE',\n",
    "                   'task_type': \"CPU\",\n",
    "                   'iterations': 5000,\n",
    "                   'od_type': \"Iter\",\n",
    "                   'bagging_temperature': 5.829866022491278, \n",
    "                   'colsample_bylevel': 0.9974218284384424, \n",
    "                   'depth': 6,\n",
    "                   'l2_leaf_reg': 14.425453774616713,\n",
    "                   'use_best_model': True\n",
    "                    }\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "class Nn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.get_feature_names()]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        print(train_df[features].shape)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class Cnn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        self.create_feat_2d(features)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def create_feat_2d(self, features, n_feats_repeat=50):\n",
    "        self.n_feats = len(features)\n",
    "        self.n_feats_repeat = n_feats_repeat\n",
    "        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n",
    "        for i in range(self.n_feats_repeat):\n",
    "            l = list(range(self.n_feats))\n",
    "            for j in range(self.n_feats):\n",
    "                c = l.pop(choice(range(len(l))))\n",
    "                self.mask[i, j] = c\n",
    "        self.mask = tf.convert_to_tensor(self.mask)\n",
    "        print(self.mask.shape)\n",
    "       \n",
    "        \n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape=(self.n_feats))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n",
    "        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        out = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        model = tf.keras.Model(inp, out)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n",
      "Reading specs.csv file....\n",
      "Specs.csv file have 386 rows and 3 columns\n",
      "Reading sample_submission.csv file....\n",
      "Sample_submission.csv file have 1000 rows and 2 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa05092a461412f9380cbf0680dd53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c6accbf7534927ae2c08fa4cc8fb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n",
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\n",
    "reduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Magma Peak - Level 1_2000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Magma Peak - Level 1_2000'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a537352d8c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmsre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mstract_hists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Magma Peak - Level 1_2000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-a537352d8c10>\u001b[0m in \u001b[0;36mstract_hists\u001b[0;34m(feature, train, test, adjust, plot)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstract_hists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madjust\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Magma Peak - Level 1_2000'"
     ]
    }
   ],
   "source": [
    "def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n",
    "        plt.show()\n",
    "    return msre\n",
    "stract_hists('Magma Peak - Level 1_2000', adjust=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call feature engineering function\n",
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in ['accuracy_group', 'installation_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: FEAT_A: Clip FEAT_B: 27253bdc - Correlation: 0.9999999999999999\n",
      "2: FEAT_A: 2050 FEAT_B: 2040 - Correlation: 0.9965259434878118\n",
      "3: FEAT_A: 2050 FEAT_B: dcaede90 - Correlation: 0.9965259434878118\n",
      "4: FEAT_A: 2050 FEAT_B: 37c53127 - Correlation: 1.0\n",
      "5: FEAT_A: 2050 FEAT_B: 73757a5e - Correlation: 0.9998050146713992\n",
      "6: FEAT_A: 2050 FEAT_B: 2b9272f4 - Correlation: 0.9999839030068793\n",
      "7: FEAT_A: 2050 FEAT_B: 26fd2d99 - Correlation: 0.9965084543995759\n",
      "8: FEAT_A: 2050 FEAT_B: 08fd73f3 - Correlation: 0.9966123918733654\n",
      "9: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_3021 - Correlation: 0.9998050146713992\n",
      "10: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_3121 - Correlation: 0.9999839030068793\n",
      "11: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2050 - Correlation: 1.0\n",
      "12: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2040 - Correlation: 0.9965259434878118\n",
      "13: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2020 - Correlation: 0.9965084543995759\n",
      "14: FEAT_A: 2050 FEAT_B: Scrub_A_Dub_2030 - Correlation: 0.9966123918733654\n",
      "15: FEAT_A: 4230 FEAT_B: 4235 - Correlation: 0.9999995197498746\n",
      "16: FEAT_A: 4230 FEAT_B: 85de926c - Correlation: 0.9999995197498746\n",
      "17: FEAT_A: 4230 FEAT_B: ad148f58 - Correlation: 0.9999999999999998\n",
      "18: FEAT_A: 4230 FEAT_B: Bubble_Bath_4230 - Correlation: 0.9999999999999998\n",
      "19: FEAT_A: 4230 FEAT_B: Bubble_Bath_4235 - Correlation: 0.9999995197498746\n",
      "20: FEAT_A: 5000 FEAT_B: 5010 - Correlation: 0.9991849213605333\n",
      "21: FEAT_A: 5000 FEAT_B: 71e712d8 - Correlation: 0.9991849213605333\n",
      "22: FEAT_A: 5000 FEAT_B: a6d66e51 - Correlation: 1.0\n",
      "23: FEAT_A: 5000 FEAT_B: Watering_Hole__Activity__5010 - Correlation: 0.9991849213605333\n",
      "24: FEAT_A: 5000 FEAT_B: Watering_Hole__Activity__5000 - Correlation: 1.0\n",
      "25: FEAT_A: 3110 FEAT_B: 3010 - Correlation: 0.9999293402893735\n",
      "26: FEAT_A: 3120 FEAT_B: 3020 - Correlation: 0.9998761417908972\n",
      "27: FEAT_A: 3121 FEAT_B: 3021 - Correlation: 0.9999098200487934\n",
      "28: FEAT_A: 4031 FEAT_B: 1996c610 - Correlation: 1.0\n",
      "29: FEAT_A: 4031 FEAT_B: Dino_Drink_4031 - Correlation: 1.0\n",
      "30: FEAT_A: 2000 FEAT_B: installation_session_count - Correlation: 1.0\n",
      "31: FEAT_A: 4050 FEAT_B: a1192f43 - Correlation: 0.9999999999999999\n",
      "32: FEAT_A: 4050 FEAT_B: Crystals_Rule_4050 - Correlation: 0.9999999999999999\n",
      "33: FEAT_A: 2020 FEAT_B: 2030 - Correlation: 0.9959933262816534\n",
      "34: FEAT_A: 4220 FEAT_B: 1340b8d7 - Correlation: 1.0\n",
      "35: FEAT_A: 4220 FEAT_B: Bubble_Bath_4220 - Correlation: 1.0\n",
      "36: FEAT_A: 44cb4907 FEAT_B: 8b757ab8 - Correlation: 0.999835058794711\n",
      "37: FEAT_A: 44cb4907 FEAT_B: Crystals_Rule_3020 - Correlation: 1.0\n",
      "38: FEAT_A: 44cb4907 FEAT_B: Crystals_Rule_3120 - Correlation: 0.999835058794711\n",
      "39: FEAT_A: 9c5ef70c FEAT_B: Pan_Balance_2000 - Correlation: 1.0\n",
      "40: FEAT_A: a1e4395d FEAT_B: a52b92d5 - Correlation: 0.9991003891313368\n",
      "41: FEAT_A: a1e4395d FEAT_B: Mushroom_Sorter__Assessment__3010 - Correlation: 1.0\n",
      "42: FEAT_A: a1e4395d FEAT_B: Mushroom_Sorter__Assessment__3110 - Correlation: 0.9991003891313368\n",
      "43: FEAT_A: 1375ccb7 FEAT_B: bdf49a58 - Correlation: 0.9993801763820348\n",
      "44: FEAT_A: 1375ccb7 FEAT_B: Bird_Measurer__Assessment__3010 - Correlation: 1.0\n",
      "45: FEAT_A: 1375ccb7 FEAT_B: Bird_Measurer__Assessment__3110 - Correlation: 0.9993801763820348\n",
      "46: FEAT_A: c54cf6c5 FEAT_B: 895865f3 - Correlation: 0.9964317167511039\n",
      "47: FEAT_A: c54cf6c5 FEAT_B: 8f094001 - Correlation: 0.9958447429109037\n",
      "48: FEAT_A: c54cf6c5 FEAT_B: 3bb91dda - Correlation: 0.9999701603895903\n",
      "49: FEAT_A: c54cf6c5 FEAT_B: d06f75b5 - Correlation: 0.9972383579301799\n",
      "50: FEAT_A: c54cf6c5 FEAT_B: Bubble_Bath_4020 - Correlation: 0.9999701603895903\n",
      "51: FEAT_A: c54cf6c5 FEAT_B: Bubble_Bath_2025 - Correlation: 1.0\n",
      "52: FEAT_A: c54cf6c5 FEAT_B: Bubble_Bath_2035 - Correlation: 0.9972383579301799\n",
      "53: FEAT_A: c54cf6c5 FEAT_B: Bubble_Bath_4045 - Correlation: 0.9958447429109037\n",
      "54: FEAT_A: c54cf6c5 FEAT_B: Bubble_Bath_2030 - Correlation: 0.9964317167511039\n",
      "55: FEAT_A: 47f43a44 FEAT_B: Flower_Waterer__Activity__4090 - Correlation: 1.0\n",
      "56: FEAT_A: 6c517a88 FEAT_B: Dino_Drink_4070 - Correlation: 1.0\n",
      "57: FEAT_A: 8fee50e2 FEAT_B: Bird_Measurer__Assessment__4020 - Correlation: 1.0\n",
      "58: FEAT_A: 67439901 FEAT_B: df4940d3 - Correlation: 0.999935162643595\n",
      "59: FEAT_A: 67439901 FEAT_B: Bottle_Filler__Activity__3010 - Correlation: 1.0\n",
      "60: FEAT_A: 67439901 FEAT_B: Bottle_Filler__Activity__3110 - Correlation: 0.999935162643595\n",
      "61: FEAT_A: 9ee1c98c FEAT_B: Sandcastle_Builder__Activity__4021 - Correlation: 0.9999999999999999\n",
      "62: FEAT_A: 857f21c0 FEAT_B: Bubble_Bath_4040 - Correlation: 1.0\n",
      "63: FEAT_A: ca11f653 FEAT_B: 1f19558b - Correlation: 0.998316427341082\n",
      "64: FEAT_A: ca11f653 FEAT_B: daac11b0 - Correlation: 0.9995251307611354\n",
      "65: FEAT_A: ca11f653 FEAT_B: All_Star_Sorting_3021 - Correlation: 0.9995251307611354\n",
      "66: FEAT_A: ca11f653 FEAT_B: All_Star_Sorting_2030 - Correlation: 1.0\n",
      "67: FEAT_A: ca11f653 FEAT_B: All_Star_Sorting_3121 - Correlation: 0.998316427341082\n",
      "68: FEAT_A: f71c4741 FEAT_B: f7e47413 - Correlation: 0.9999426890770878\n",
      "69: FEAT_A: f71c4741 FEAT_B: Scrub_A_Dub_3110 - Correlation: 0.9999426890770878\n",
      "70: FEAT_A: f71c4741 FEAT_B: Scrub_A_Dub_3010 - Correlation: 1.0\n",
      "71: FEAT_A: 29a42aea FEAT_B: Bubble_Bath_4080 - Correlation: 1.0\n",
      "72: FEAT_A: 89aace00 FEAT_B: e5734469 - Correlation: 0.9998406115110345\n",
      "73: FEAT_A: 89aace00 FEAT_B: Dino_Drink_3020 - Correlation: 0.9998406115110345\n",
      "74: FEAT_A: 89aace00 FEAT_B: Dino_Drink_3120 - Correlation: 1.0\n",
      "75: FEAT_A: 14de4c5d FEAT_B: Air_Show_4100 - Correlation: 1.0\n",
      "76: FEAT_A: 31973d56 FEAT_B: 5de79a6a - Correlation: 0.9973945339840646\n",
      "77: FEAT_A: 31973d56 FEAT_B: Cart_Balancer__Assessment__3020 - Correlation: 0.9973945339840646\n",
      "78: FEAT_A: 31973d56 FEAT_B: Cart_Balancer__Assessment__3120 - Correlation: 1.0\n",
      "79: FEAT_A: 363d3849 FEAT_B: 9e4c8c7b - Correlation: 0.9992130941883633\n",
      "80: FEAT_A: 363d3849 FEAT_B: All_Star_Sorting_3110 - Correlation: 0.9992130941883633\n",
      "81: FEAT_A: 363d3849 FEAT_B: All_Star_Sorting_3010 - Correlation: 1.0\n",
      "82: FEAT_A: c58186bf FEAT_B: Sandcastle_Builder__Activity__4035 - Correlation: 1.0\n",
      "83: FEAT_A: 0a08139c FEAT_B: 71fe8f75 - Correlation: 0.9999850342981554\n",
      "84: FEAT_A: 0a08139c FEAT_B: Bug_Measurer__Activity__3110 - Correlation: 0.9999850342981554\n",
      "85: FEAT_A: 0a08139c FEAT_B: Bug_Measurer__Activity__3010 - Correlation: 1.0\n",
      "86: FEAT_A: 832735e1 FEAT_B: ab3136ba - Correlation: 0.9998637945770242\n",
      "87: FEAT_A: 832735e1 FEAT_B: Dino_Dive_3010 - Correlation: 1.0\n",
      "88: FEAT_A: 832735e1 FEAT_B: Dino_Dive_3110 - Correlation: 0.9998637945770242\n",
      "89: FEAT_A: 2b058fe3 FEAT_B: Cauldron_Filler__Assessment__2010 - Correlation: 1.0\n",
      "90: FEAT_A: 0d18d96c FEAT_B: Mushroom_Sorter__Assessment__4035 - Correlation: 1.0\n",
      "91: FEAT_A: 3ccd3f02 FEAT_B: 3dcdda7f - Correlation: 0.9977337946782758\n",
      "92: FEAT_A: 3ccd3f02 FEAT_B: Chest_Sorter__Assessment__3110 - Correlation: 1.0\n",
      "93: FEAT_A: 3ccd3f02 FEAT_B: Chest_Sorter__Assessment__3010 - Correlation: 0.9977337946782758\n",
      "94: FEAT_A: 598f4598 FEAT_B: Flower_Waterer__Activity__4025 - Correlation: 0.9999999999999998\n",
      "95: FEAT_A: 91561152 FEAT_B: Cauldron_Filler__Assessment__4025 - Correlation: 1.0\n",
      "96: FEAT_A: 58a0de5c FEAT_B: 9b4001e4 - Correlation: 0.9999159362216189\n",
      "97: FEAT_A: 58a0de5c FEAT_B: f5b8c21a - Correlation: 0.9977888184537715\n",
      "98: FEAT_A: 58a0de5c FEAT_B: Air_Show_3121 - Correlation: 1.0\n",
      "99: FEAT_A: 58a0de5c FEAT_B: Air_Show_3021 - Correlation: 0.9999159362216189\n",
      "100: FEAT_A: 58a0de5c FEAT_B: Air_Show_2030 - Correlation: 0.9977888184537715\n",
      "101: FEAT_A: a2df0760 FEAT_B: Happy_Camel_4035 - Correlation: 0.9999999999999999\n",
      "102: FEAT_A: 155f62a4 FEAT_B: 5b49460a - Correlation: 0.9999999999999999\n",
      "103: FEAT_A: 155f62a4 FEAT_B: Chest_Sorter__Assessment__2000 - Correlation: 0.9999999999999999\n",
      "104: FEAT_A: 155f62a4 FEAT_B: Chest_Sorter__Assessment__2020 - Correlation: 0.9999999999999999\n",
      "105: FEAT_A: 5f5b2617 FEAT_B: Bottle_Filler__Activity__4080 - Correlation: 1.0\n",
      "106: FEAT_A: 9b01374f FEAT_B: Flower_Waterer__Activity__2000 - Correlation: 1.0\n",
      "107: FEAT_A: 86ba578b FEAT_B: Leaf_Leader_2070 - Correlation: 1.0\n",
      "108: FEAT_A: e5c9df6f FEAT_B: b012cd7f - Correlation: 0.9990885395773585\n",
      "109: FEAT_A: e5c9df6f FEAT_B: 3afde5dd - Correlation: 0.9990910601838011\n",
      "110: FEAT_A: e5c9df6f FEAT_B: Leaf_Leader_3121 - Correlation: 1.0\n",
      "111: FEAT_A: e5c9df6f FEAT_B: Leaf_Leader_2030 - Correlation: 0.9990885395773585\n",
      "112: FEAT_A: e5c9df6f FEAT_B: Leaf_Leader_3021 - Correlation: 0.9990910601838011\n",
      "113: FEAT_A: 46b50ba8 FEAT_B: Happy_Camel_4095 - Correlation: 0.9999999999999998\n",
      "114: FEAT_A: 48349b14 FEAT_B: Crystals_Rule_2000 - Correlation: 1.0\n",
      "115: FEAT_A: 0db6d71d FEAT_B: Chest_Sorter__Assessment__4020 - Correlation: 1.0\n",
      "116: FEAT_A: 3bf1cf26 FEAT_B: 1af8be29 - Correlation: 0.9998900847287077\n",
      "117: FEAT_A: 3bf1cf26 FEAT_B: Happy_Camel_3020 - Correlation: 0.9998900847287077\n",
      "118: FEAT_A: 3bf1cf26 FEAT_B: Happy_Camel_3120 - Correlation: 0.9999999999999999\n",
      "119: FEAT_A: 7040c096 FEAT_B: Scrub_A_Dub_4010 - Correlation: 1.0\n",
      "120: FEAT_A: 7d5c30a2 FEAT_B: Dino_Dive_2060 - Correlation: 1.0\n",
      "121: FEAT_A: 38074c54 FEAT_B: 222660ff - Correlation: 0.9999999999999998\n",
      "122: FEAT_A: 38074c54 FEAT_B: e4f1efe6 - Correlation: 0.9984044689083383\n",
      "123: FEAT_A: 38074c54 FEAT_B: Chest_Sorter__Assessment__3121 - Correlation: 0.9984044689083383\n",
      "124: FEAT_A: 38074c54 FEAT_B: Chest_Sorter__Assessment__2030 - Correlation: 0.9999999999999998\n",
      "125: FEAT_A: 38074c54 FEAT_B: Chest_Sorter__Assessment__2010 - Correlation: 0.9999999999999998\n",
      "126: FEAT_A: 461eace6 FEAT_B: Egg_Dropper__Activity__4020 - Correlation: 1.0\n",
      "127: FEAT_A: cb1178ad FEAT_B: Chest_Sorter__Assessment__4090 - Correlation: 1.0\n",
      "128: FEAT_A: e7561dd2 FEAT_B: Pan_Balance_4025 - Correlation: 0.9999999999999998\n",
      "129: FEAT_A: b74258a0 FEAT_B: ecaab346 - Correlation: 1.0\n",
      "130: FEAT_A: b74258a0 FEAT_B: b2e5b0f1 - Correlation: 0.999849464604504\n",
      "131: FEAT_A: b74258a0 FEAT_B: Cart_Balancer__Assessment__3121 - Correlation: 1.0\n",
      "132: FEAT_A: b74258a0 FEAT_B: Cart_Balancer__Assessment__2010 - Correlation: 0.999849464604504\n",
      "133: FEAT_A: b74258a0 FEAT_B: Cart_Balancer__Assessment__2030 - Correlation: 1.0\n",
      "134: FEAT_A: 4d911100 FEAT_B: 77ead60d - Correlation: 0.9998475927724766\n",
      "135: FEAT_A: 4d911100 FEAT_B: 16dffff1 - Correlation: 0.9986046680098603\n",
      "136: FEAT_A: 4d911100 FEAT_B: Dino_Drink_2030 - Correlation: 0.9986046680098603\n",
      "137: FEAT_A: 4d911100 FEAT_B: Dino_Drink_3121 - Correlation: 1.0\n",
      "138: FEAT_A: 4d911100 FEAT_B: Dino_Drink_3021 - Correlation: 0.9998475927724766\n",
      "139: FEAT_A: 83c6c409 FEAT_B: 9d29771f - Correlation: 0.9963745894369906\n",
      "140: FEAT_A: 83c6c409 FEAT_B: 3dfd4aa4 - Correlation: 0.9999970893851744\n",
      "141: FEAT_A: 83c6c409 FEAT_B: 28ed704e - Correlation: 0.9996646794414074\n",
      "142: FEAT_A: 83c6c409 FEAT_B: c74f40cd - Correlation: 0.9963987221516345\n",
      "143: FEAT_A: 83c6c409 FEAT_B: Mushroom_Sorter__Assessment__3121 - Correlation: 0.9963987221516345\n",
      "144: FEAT_A: 83c6c409 FEAT_B: Mushroom_Sorter__Assessment__2020 - Correlation: 0.9999970893851744\n",
      "145: FEAT_A: 83c6c409 FEAT_B: Mushroom_Sorter__Assessment__2035 - Correlation: 1.0\n",
      "146: FEAT_A: 83c6c409 FEAT_B: Mushroom_Sorter__Assessment__3021 - Correlation: 0.9963745894369906\n",
      "147: FEAT_A: 83c6c409 FEAT_B: Mushroom_Sorter__Assessment__4025 - Correlation: 0.9996646794414074\n",
      "148: FEAT_A: 7ab78247 FEAT_B: b80e5e84 - Correlation: 0.9998336590281087\n",
      "149: FEAT_A: 7ab78247 FEAT_B: Egg_Dropper__Activity__3010 - Correlation: 1.0\n",
      "150: FEAT_A: 7ab78247 FEAT_B: Egg_Dropper__Activity__3110 - Correlation: 0.9998336590281087\n",
      "151: FEAT_A: 3d0b9317 FEAT_B: Chest_Sorter__Assessment__4040 - Correlation: 1.0\n",
      "152: FEAT_A: 611485c5 FEAT_B: Fireworks__Activity__4080 - Correlation: 1.0\n",
      "153: FEAT_A: 2c4e6db0 FEAT_B: All_Star_Sorting_2020 - Correlation: 1.0\n",
      "154: FEAT_A: 4a09ace1 FEAT_B: Scrub_A_Dub_2083 - Correlation: 1.0\n",
      "155: FEAT_A: 47efca07 FEAT_B: Bottle_Filler__Activity__4090 - Correlation: 1.0\n",
      "156: FEAT_A: 4bb2f698 FEAT_B: Chicken_Balancer__Activity__4070 - Correlation: 0.9999999999999998\n",
      "157: FEAT_A: 90efca10 FEAT_B: Bottle_Filler__Activity__4020 - Correlation: 1.0\n",
      "158: FEAT_A: e720d930 FEAT_B: 3323d7e9 - Correlation: 0.9998454394844453\n",
      "159: FEAT_A: e720d930 FEAT_B: 7cf1bc53 - Correlation: 0.9979345979643147\n",
      "160: FEAT_A: e720d930 FEAT_B: 3ddc79c3 - Correlation: 0.9998920962508024\n",
      "161: FEAT_A: e720d930 FEAT_B: Crystals_Rule_2030 - Correlation: 0.9998454394844453\n",
      "162: FEAT_A: e720d930 FEAT_B: Crystals_Rule_3021 - Correlation: 0.9998920962508024\n",
      "163: FEAT_A: e720d930 FEAT_B: Crystals_Rule_2020 - Correlation: 0.9979345979643147\n",
      "164: FEAT_A: e720d930 FEAT_B: Crystals_Rule_3121 - Correlation: 1.0\n",
      "165: FEAT_A: abc5811c FEAT_B: Happy_Camel_4010 - Correlation: 1.0\n",
      "166: FEAT_A: 9ce586dd FEAT_B: Chest_Sorter__Assessment__4035 - Correlation: 1.0\n",
      "167: FEAT_A: e694a35b FEAT_B: Fireworks__Activity__4020 - Correlation: 1.0\n",
      "168: FEAT_A: 84b0e0c8 FEAT_B: ea321fb1 - Correlation: 0.9993007600205109\n",
      "169: FEAT_A: 84b0e0c8 FEAT_B: Chicken_Balancer__Activity__3010 - Correlation: 0.9993007600205109\n",
      "170: FEAT_A: 84b0e0c8 FEAT_B: Chicken_Balancer__Activity__3110 - Correlation: 1.0\n",
      "171: FEAT_A: 92687c59 FEAT_B: Scrub_A_Dub_4090 - Correlation: 1.0\n",
      "172: FEAT_A: d88ca108 FEAT_B: Air_Show_2070 - Correlation: 1.0\n",
      "173: FEAT_A: e64e2cfd FEAT_B: Watering_Hole__Activity__2000 - Correlation: 0.9999999999999998\n",
      "174: FEAT_A: 0d1da71f FEAT_B: Chow_Time_3110 - Correlation: 1.0\n",
      "175: FEAT_A: a8efe47b FEAT_B: Chest_Sorter__Assessment__4030 - Correlation: 1.0\n",
      "176: FEAT_A: a44b10dc FEAT_B: Flower_Waterer__Activity__4070 - Correlation: 1.0\n",
      "177: FEAT_A: a8876db3 FEAT_B: Cart_Balancer__Assessment__3021 - Correlation: 1.0\n",
      "178: FEAT_A: 119b5b02 FEAT_B: Dino_Dive_4080 - Correlation: 1.0\n",
      "179: FEAT_A: 30614231 FEAT_B: 37ee8496 - Correlation: 0.9967763987631819\n",
      "180: FEAT_A: 30614231 FEAT_B: Cauldron_Filler__Assessment__4020 - Correlation: 1.0\n",
      "181: FEAT_A: 30614231 FEAT_B: Cauldron_Filler__Assessment__4030 - Correlation: 0.9967763987631819\n",
      "182: FEAT_A: fd20ea40 FEAT_B: Leaf_Leader_4010 - Correlation: 0.9999999999999998\n",
      "183: FEAT_A: 262136f4 FEAT_B: Leaf_Leader_4020 - Correlation: 1.0\n",
      "184: FEAT_A: c7128948 FEAT_B: Mushroom_Sorter__Assessment__4040 - Correlation: 1.0\n",
      "185: FEAT_A: bb3e370b FEAT_B: Bottle_Filler__Activity_ - Correlation: 0.9950043311420306\n",
      "186: FEAT_A: bb3e370b FEAT_B: Bottle_Filler__Activity__4030 - Correlation: 0.9999999999999999\n",
      "187: FEAT_A: bd701df8 FEAT_B: 49ed92e9 - Correlation: 0.9993109138888533\n",
      "188: FEAT_A: bd701df8 FEAT_B: Watering_Hole__Activity__3110 - Correlation: 1.0\n",
      "189: FEAT_A: bd701df8 FEAT_B: Watering_Hole__Activity__3010 - Correlation: 0.9993109138888533\n",
      "190: FEAT_A: 6aeafed4 FEAT_B: Bubble_Bath_4090 - Correlation: 1.0\n",
      "191: FEAT_A: 9d4e7b25 FEAT_B: Cart_Balancer__Assessment__4040 - Correlation: 1.0\n",
      "192: FEAT_A: 15f99afc FEAT_B: 6cf7d25c - Correlation: 0.9994848234397947\n",
      "193: FEAT_A: 15f99afc FEAT_B: Pan_Balance_3110 - Correlation: 1.0\n",
      "194: FEAT_A: 15f99afc FEAT_B: Pan_Balance_3010 - Correlation: 0.9994848234397947\n",
      "195: FEAT_A: 6f445b57 FEAT_B: Chow_Time_4080 - Correlation: 1.0\n",
      "196: FEAT_A: 5859dfb6 FEAT_B: 90ea0bac - Correlation: 0.998105247261057\n",
      "197: FEAT_A: 5859dfb6 FEAT_B: Bubble_Bath_3120 - Correlation: 0.9999999999999998\n",
      "198: FEAT_A: 5859dfb6 FEAT_B: Bubble_Bath_3020 - Correlation: 0.998105247261057\n",
      "199: FEAT_A: 4a4c3d21 FEAT_B: Bird_Measurer__Assessment__4025 - Correlation: 1.0\n",
      "200: FEAT_A: 1cf54632 FEAT_B: Bubble_Bath_2000 - Correlation: 0.9999999999999999\n",
      "201: FEAT_A: 1575e76c FEAT_B: Air_Show_2020 - Correlation: 1.0\n",
      "202: FEAT_A: 36fa3ebe FEAT_B: c7fe2a55 - Correlation: 0.9998350234117662\n",
      "203: FEAT_A: 36fa3ebe FEAT_B: a8a78786 - Correlation: 0.9979559120623818\n",
      "204: FEAT_A: 36fa3ebe FEAT_B: Happy_Camel_3021 - Correlation: 0.9998350234117662\n",
      "205: FEAT_A: 36fa3ebe FEAT_B: Happy_Camel_3121 - Correlation: 0.9979559120623818\n",
      "206: FEAT_A: 36fa3ebe FEAT_B: Happy_Camel_2030 - Correlation: 1.0\n",
      "207: FEAT_A: 6c930e6e FEAT_B: a5be6304 - Correlation: 0.9962760616821671\n",
      "208: FEAT_A: 6c930e6e FEAT_B: Mushroom_Sorter__Assessment__2010 - Correlation: 0.9962760616821671\n",
      "209: FEAT_A: 6c930e6e FEAT_B: Mushroom_Sorter__Assessment__2030 - Correlation: 0.9999999999999999\n",
      "210: FEAT_A: d2278a3b FEAT_B: Bottle_Filler__Activity__2000 - Correlation: 1.0\n",
      "211: FEAT_A: 7423acbc FEAT_B: e04fb33d - Correlation: 0.999628997408126\n",
      "212: FEAT_A: 7423acbc FEAT_B: Air_Show_3020 - Correlation: 1.0\n",
      "213: FEAT_A: 7423acbc FEAT_B: Air_Show_3120 - Correlation: 0.999628997408126\n",
      "214: FEAT_A: b120f2ac FEAT_B: d45ed6a1 - Correlation: 0.9979707847816691\n",
      "215: FEAT_A: b120f2ac FEAT_B: c277e121 - Correlation: 0.9999983835744553\n",
      "216: FEAT_A: b120f2ac FEAT_B: All_Star_Sorting_3020 - Correlation: 0.9999983835744553\n",
      "217: FEAT_A: b120f2ac FEAT_B: All_Star_Sorting_3120 - Correlation: 0.9979707847816691\n",
      "218: FEAT_A: b120f2ac FEAT_B: All_Star_Sorting_2025 - Correlation: 1.0\n",
      "219: FEAT_A: 7f0836bf FEAT_B: a29c5338 - Correlation: 0.9986531654717627\n",
      "220: FEAT_A: 7f0836bf FEAT_B: Dino_Drink_3010 - Correlation: 0.9986531654717627\n",
      "221: FEAT_A: 7f0836bf FEAT_B: Dino_Drink_3110 - Correlation: 1.0\n",
      "222: FEAT_A: e3ff61fb FEAT_B: 709b1251 - Correlation: 0.9995444786291265\n",
      "223: FEAT_A: e3ff61fb FEAT_B: Dino_Dive_3021 - Correlation: 1.0\n",
      "224: FEAT_A: e3ff61fb FEAT_B: Dino_Dive_3121 - Correlation: 0.9995444786291265\n",
      "225: FEAT_A: 9554a50b FEAT_B: Cauldron_Filler__Assessment__4080 - Correlation: 0.9999999999999999\n",
      "226: FEAT_A: 7961e599 FEAT_B: Dino_Dive_2020 - Correlation: 0.9999999999999999\n",
      "227: FEAT_A: 51102b85 FEAT_B: Bird_Measurer__Assessment__4030 - Correlation: 1.0\n",
      "228: FEAT_A: 1cc7cfca FEAT_B: All_Star_Sorting_4030 - Correlation: 1.0\n",
      "229: FEAT_A: cf82af56 FEAT_B: Scrub_A_Dub_4070 - Correlation: 1.0\n",
      "230: FEAT_A: 28f975ea FEAT_B: Air_Show_4020 - Correlation: 1.0\n",
      "231: FEAT_A: 5e3ea25a FEAT_B: Crystals_Rule_4070 - Correlation: 1.0\n",
      "232: FEAT_A: 37937459 FEAT_B: Sandcastle_Builder__Activity__4090 - Correlation: 0.9999999999999998\n",
      "233: FEAT_A: 7dfe6d8a FEAT_B: Leaf_Leader_4070 - Correlation: 0.9999999999999998\n",
      "234: FEAT_A: 99ea62f3 FEAT_B: Bubble_Bath_2083 - Correlation: 1.0\n",
      "235: FEAT_A: d2e9262e FEAT_B: 2fb91ec1 - Correlation: 0.9991434495208743\n",
      "236: FEAT_A: d2e9262e FEAT_B: Watering_Hole__Activity__4025 - Correlation: 0.9991434495208743\n",
      "237: FEAT_A: d2e9262e FEAT_B: Watering_Hole__Activity__4020 - Correlation: 0.9999999999999998\n",
      "238: FEAT_A: 884228c8 FEAT_B: Fireworks__Activity__4070 - Correlation: 1.0\n",
      "239: FEAT_A: e7e44842 FEAT_B: Watering_Hole__Activity__4090 - Correlation: 0.9999999999999999\n",
      "240: FEAT_A: bd612267 FEAT_B: Chest_Sorter__Assessment__4070 - Correlation: 0.9999999999999999\n",
      "241: FEAT_A: 2a512369 FEAT_B: 33505eae - Correlation: 0.9994585292841954\n",
      "242: FEAT_A: 2a512369 FEAT_B: Leaf_Leader_3010 - Correlation: 0.9994585292841954\n",
      "243: FEAT_A: 2a512369 FEAT_B: Leaf_Leader_3110 - Correlation: 1.0\n",
      "244: FEAT_A: 7525289a FEAT_B: 45d01abe - Correlation: 0.9981555049446889\n",
      "245: FEAT_A: 7525289a FEAT_B: Bird_Measurer__Assessment__3021 - Correlation: 0.9981555049446889\n",
      "246: FEAT_A: 7525289a FEAT_B: Bird_Measurer__Assessment__3121 - Correlation: 1.0\n",
      "247: FEAT_A: 63f13dd7 FEAT_B: Chow_Time_2020 - Correlation: 1.0\n",
      "248: FEAT_A: 86c924c4 FEAT_B: 5154fc30 - Correlation: 0.9988936322895169\n",
      "249: FEAT_A: 86c924c4 FEAT_B: 3babcb9b - Correlation: 0.9988970478897697\n",
      "250: FEAT_A: 86c924c4 FEAT_B: Crystals_Rule_3010 - Correlation: 0.9988936322895169\n",
      "251: FEAT_A: 86c924c4 FEAT_B: Crystals_Rule_4020 - Correlation: 1.0\n",
      "252: FEAT_A: 86c924c4 FEAT_B: Crystals_Rule_3110 - Correlation: 0.9988970478897697\n",
      "253: FEAT_A: 6088b756 FEAT_B: Dino_Dive_2070 - Correlation: 1.0\n",
      "254: FEAT_A: 6d90d394 FEAT_B: Scrub_A_Dub_2000 - Correlation: 0.9999999999999999\n",
      "255: FEAT_A: fcfdffb6 FEAT_B: Flower_Waterer__Activity__4022 - Correlation: 1.0\n",
      "256: FEAT_A: 08ff79ad FEAT_B: Egg_Dropper__Activity__4090 - Correlation: 1.0\n",
      "257: FEAT_A: 69fdac0a FEAT_B: 8d7e386c - Correlation: 0.9996590210382708\n",
      "258: FEAT_A: 69fdac0a FEAT_B: Happy_Camel_3010 - Correlation: 0.9996590210382708\n",
      "259: FEAT_A: 69fdac0a FEAT_B: Happy_Camel_3110 - Correlation: 0.9999999999999999\n",
      "260: FEAT_A: b7530680 FEAT_B: e9c52111 - Correlation: 0.998817689964623\n",
      "261: FEAT_A: b7530680 FEAT_B: Bottle_Filler__Activity__2020 - Correlation: 1.0\n",
      "262: FEAT_A: b7530680 FEAT_B: Bottle_Filler__Activity__2030 - Correlation: 0.998817689964623\n",
      "263: FEAT_A: 17113b36 FEAT_B: ad2fc29c - Correlation: 0.9988424704659317\n",
      "264: FEAT_A: 17113b36 FEAT_B: e37a2b78 - Correlation: 0.9982548122198927\n",
      "265: FEAT_A: 17113b36 FEAT_B: Bird_Measurer__Assessment__4110 - Correlation: 0.9999999999999998\n",
      "266: FEAT_A: 17113b36 FEAT_B: Bird_Measurer__Assessment__3020 - Correlation: 0.9988424704659317\n",
      "267: FEAT_A: 17113b36 FEAT_B: Bird_Measurer__Assessment__3120 - Correlation: 0.9982548122198927\n",
      "268: FEAT_A: a592d54e FEAT_B: 250513af - Correlation: 0.9973020736885158\n",
      "269: FEAT_A: a592d54e FEAT_B: cf7638f3 - Correlation: 0.9969813738295747\n",
      "270: FEAT_A: a592d54e FEAT_B: 1c178d24 - Correlation: 0.9973125883100831\n",
      "271: FEAT_A: a592d54e FEAT_B: Pan_Balance_3021 - Correlation: 0.9973020736885158\n",
      "272: FEAT_A: a592d54e FEAT_B: Pan_Balance_2020 - Correlation: 0.9999999999999999\n",
      "273: FEAT_A: a592d54e FEAT_B: Pan_Balance_2030 - Correlation: 0.9973125883100831\n",
      "274: FEAT_A: a592d54e FEAT_B: Pan_Balance_3121 - Correlation: 0.9969813738295747\n",
      "275: FEAT_A: 562cec5f FEAT_B: Chest_Sorter__Assessment__4025 - Correlation: 1.0\n",
      "276: FEAT_A: 9e6b7fb5 FEAT_B: Chow_Time_4095 - Correlation: 0.9999999999999998\n",
      "277: FEAT_A: 00c73085 FEAT_B: Dino_Dive_2030 - Correlation: 0.9999999999999999\n",
      "278: FEAT_A: d38c2fd7 FEAT_B: Bird_Measurer__Assessment__4035 - Correlation: 0.9999999999999999\n",
      "279: FEAT_A: c6971acf FEAT_B: Dino_Drink_2060 - Correlation: 0.9999999999999999\n",
      "280: FEAT_A: e080a381 FEAT_B: Pan_Balance_4090 - Correlation: 1.0\n",
      "281: FEAT_A: 87d743c1 FEAT_B: Dino_Dive_4010 - Correlation: 1.0\n",
      "282: FEAT_A: 5e109ec3 FEAT_B: Cart_Balancer__Assessment__4030 - Correlation: 1.0\n",
      "283: FEAT_A: 907a054b FEAT_B: c51d8688 - Correlation: 0.9999667370361688\n",
      "284: FEAT_A: 907a054b FEAT_B: Pan_Balance_3020 - Correlation: 1.0\n",
      "285: FEAT_A: 907a054b FEAT_B: Pan_Balance_3120 - Correlation: 0.9999667370361688\n",
      "286: FEAT_A: 99abe2bb FEAT_B: Bubble_Bath_2080 - Correlation: 1.0\n",
      "287: FEAT_A: 56817e2b FEAT_B: 47026d5f - Correlation: 0.9993683693704368\n",
      "288: FEAT_A: 56817e2b FEAT_B: cb6010f8 - Correlation: 0.9990475739429782\n",
      "289: FEAT_A: 56817e2b FEAT_B: Chow_Time_2030 - Correlation: 1.0\n",
      "290: FEAT_A: 56817e2b FEAT_B: Chow_Time_3121 - Correlation: 0.9990475739429782\n",
      "291: FEAT_A: 56817e2b FEAT_B: Chow_Time_3021 - Correlation: 0.9993683693704368\n",
      "292: FEAT_A: 3bb91ced FEAT_B: Happy_Camel_2081 - Correlation: 1.0\n",
      "293: FEAT_A: 3bfd1a65 FEAT_B: db02c830 - Correlation: 0.9999982205265872\n",
      "294: FEAT_A: 3bfd1a65 FEAT_B: Mushroom_Sorter__Assessment__2000 - Correlation: 1.0\n",
      "295: FEAT_A: 3bfd1a65 FEAT_B: Mushroom_Sorter__Assessment__2025 - Correlation: 0.9999982205265872\n",
      "296: FEAT_A: 74e5f8a7 FEAT_B: Dino_Drink_4020 - Correlation: 1.0\n",
      "297: FEAT_A: acf5c23f FEAT_B: Cart_Balancer__Assessment__4070 - Correlation: 1.0\n",
      "298: FEAT_A: a76029ee FEAT_B: Bird_Measurer__Assessment__4040 - Correlation: 0.9999999999999999\n",
      "299: FEAT_A: c189aaf2 FEAT_B: Happy_Camel_2083 - Correlation: 0.9999999999999999\n",
      "300: FEAT_A: 16667cc5 FEAT_B: Chicken_Balancer__Activity__4080 - Correlation: 0.9999999999999999\n",
      "301: FEAT_A: 0413e89d FEAT_B: 15eb4a7d - Correlation: 0.9997266832893074\n",
      "302: FEAT_A: 0413e89d FEAT_B: Bubble_Bath_3110 - Correlation: 0.9997266832893074\n",
      "303: FEAT_A: 0413e89d FEAT_B: Bubble_Bath_3010 - Correlation: 1.0\n",
      "304: FEAT_A: 56cd3b43 FEAT_B: bbfe0445 - Correlation: 0.9996926215355526\n",
      "305: FEAT_A: 56cd3b43 FEAT_B: Flower_Waterer__Activity__3110 - Correlation: 0.9996926215355526\n",
      "306: FEAT_A: 56cd3b43 FEAT_B: Flower_Waterer__Activity__3010 - Correlation: 1.0\n",
      "307: FEAT_A: eb2c19cd FEAT_B: Mushroom_Sorter__Assessment__4090 - Correlation: 1.0\n",
      "308: FEAT_A: a16a373e FEAT_B: Bird_Measurer__Assessment__4070 - Correlation: 1.0\n",
      "309: FEAT_A: 7fd1ac25 FEAT_B: Egg_Dropper__Activity__4080 - Correlation: 1.0\n",
      "310: FEAT_A: bcceccc6 FEAT_B: Air_Show_4070 - Correlation: 1.0\n",
      "311: FEAT_A: ea296733 FEAT_B: df4fe8b6 - Correlation: 0.9972489515829078\n",
      "312: FEAT_A: ea296733 FEAT_B: Chest_Sorter__Assessment__3120 - Correlation: 0.9972489515829078\n",
      "313: FEAT_A: ea296733 FEAT_B: Chest_Sorter__Assessment__3020 - Correlation: 1.0\n",
      "314: FEAT_A: 4d6737eb FEAT_B: Dino_Drink_2070 - Correlation: 1.0\n",
      "315: FEAT_A: 15ba1109 FEAT_B: Air_Show_2000 - Correlation: 1.0\n",
      "316: FEAT_A: a0faea5d FEAT_B: Bubble_Bath_4070 - Correlation: 1.0\n",
      "317: FEAT_A: d88e8f25 FEAT_B: ac92046e - Correlation: 0.9999763070332106\n",
      "318: FEAT_A: d88e8f25 FEAT_B: Scrub_A_Dub_3020 - Correlation: 1.0\n",
      "319: FEAT_A: d88e8f25 FEAT_B: Scrub_A_Dub_3120 - Correlation: 0.9999763070332106\n",
      "320: FEAT_A: ecc36b7f FEAT_B: Bubble_Bath_4095 - Correlation: 1.0\n",
      "321: FEAT_A: dcb55a27 FEAT_B: Air_Show_4110 - Correlation: 1.0\n",
      "322: FEAT_A: 04df9b66 FEAT_B: 5290eab1 - Correlation: 0.9998190477466209\n",
      "323: FEAT_A: 04df9b66 FEAT_B: Cauldron_Filler__Assessment__3120 - Correlation: 0.9998190477466209\n",
      "324: FEAT_A: 04df9b66 FEAT_B: Cauldron_Filler__Assessment__3020 - Correlation: 1.0\n",
      "325: FEAT_A: f56e0afc FEAT_B: Bird_Measurer__Assessment__2000 - Correlation: 1.0\n",
      "326: FEAT_A: c7f7f0e1 FEAT_B: Bug_Measurer__Activity__2000 - Correlation: 1.0\n",
      "327: FEAT_A: 0330ab6a FEAT_B: 2230fab4 - Correlation: 0.9998673365188063\n",
      "328: FEAT_A: 0330ab6a FEAT_B: Chow_Time_3020 - Correlation: 1.0\n",
      "329: FEAT_A: 0330ab6a FEAT_B: Chow_Time_3120 - Correlation: 0.9998673365188063\n",
      "330: FEAT_A: 2dc29e21 FEAT_B: All_Star_Sorting_4020 - Correlation: 1.0\n",
      "331: FEAT_A: 7ec0c298 FEAT_B: Chow_Time_3010 - Correlation: 0.9999999999999999\n",
      "332: FEAT_A: 7372e1a5 FEAT_B: Chow_Time_4070 - Correlation: 1.0\n",
      "333: FEAT_A: 56bcd38d FEAT_B: Chicken_Balancer__Activity__4030 - Correlation: 0.9999999999999998\n",
      "334: FEAT_A: c2baf0bd FEAT_B: Happy_Camel_2020 - Correlation: 1.0\n",
      "335: FEAT_A: a5e9da97 FEAT_B: Pan_Balance_4100 - Correlation: 1.0\n",
      "336: FEAT_A: 15a43e5b FEAT_B: Bottle_Filler__Activity__4070 - Correlation: 1.0\n",
      "337: FEAT_A: 55115cbd FEAT_B: 6f4adc4b - Correlation: 0.9997831892615398\n",
      "338: FEAT_A: 55115cbd FEAT_B: Bubble_Bath_3121 - Correlation: 0.9999999999999999\n",
      "339: FEAT_A: 55115cbd FEAT_B: Bubble_Bath_3021 - Correlation: 0.9997831892615398\n",
      "340: FEAT_A: 2dcad279 FEAT_B: 923afab1 - Correlation: 0.9998567985670083\n",
      "341: FEAT_A: 2dcad279 FEAT_B: Cauldron_Filler__Assessment__3110 - Correlation: 0.9999999999999999\n",
      "342: FEAT_A: 2dcad279 FEAT_B: Cauldron_Filler__Assessment__3010 - Correlation: 0.9998567985670083\n",
      "343: FEAT_A: f3cd5473 FEAT_B: Pan_Balance_4070 - Correlation: 1.0\n",
      "344: FEAT_A: a1bbe385 FEAT_B: f28c589a - Correlation: 0.999953679734225\n",
      "345: FEAT_A: a1bbe385 FEAT_B: Air_Show_3110 - Correlation: 1.0\n",
      "346: FEAT_A: a1bbe385 FEAT_B: Air_Show_3010 - Correlation: 0.999953679734225\n",
      "347: FEAT_A: 3edf6747 FEAT_B: Cauldron_Filler__Assessment__4035 - Correlation: 0.9999999999999999\n",
      "348: FEAT_A: 8af75982 FEAT_B: Happy_Camel_4020 - Correlation: 1.0\n",
      "349: FEAT_A: 5348fd84 FEAT_B: Cauldron_Filler__Assessment__4040 - Correlation: 1.0\n",
      "350: FEAT_A: de26c3a6 FEAT_B: Flower_Waterer__Activity__4020 - Correlation: 0.9999999999999999\n",
      "351: FEAT_A: 65abac75 FEAT_B: Air_Show_4010 - Correlation: 1.0\n",
      "352: FEAT_A: f806dc10 FEAT_B: Dino_Drink_2020 - Correlation: 1.0\n",
      "353: FEAT_A: 3b2048ee FEAT_B: Leaf_Leader_4095 - Correlation: 1.0\n",
      "354: FEAT_A: f93fc684 FEAT_B: Chow_Time_4010 - Correlation: 0.9999999999999999\n",
      "355: FEAT_A: 6f8106d9 FEAT_B: Dino_Drink_4090 - Correlation: 1.0\n",
      "356: FEAT_A: 532a2afb FEAT_B: Cauldron_Filler__Assessment__2020 - Correlation: 1.0\n",
      "357: FEAT_A: 77c76bc5 FEAT_B: Cauldron_Filler__Assessment__4090 - Correlation: 1.0\n",
      "358: FEAT_A: 1bb5fbdb FEAT_B: b2dba42b - Correlation: 0.9999521729413294\n",
      "359: FEAT_A: 1bb5fbdb FEAT_B: Sandcastle_Builder__Activity__3010 - Correlation: 0.9999521729413294\n",
      "360: FEAT_A: 1bb5fbdb FEAT_B: Sandcastle_Builder__Activity__3110 - Correlation: 1.0\n",
      "361: FEAT_A: 8d748b58 FEAT_B: Bug_Measurer__Activity__4090 - Correlation: 0.9999999999999998\n",
      "362: FEAT_A: 804ee27f FEAT_B: Pan_Balance_4020 - Correlation: 1.0\n",
      "363: FEAT_A: 499edb7c FEAT_B: Chicken_Balancer__Activity__4020 - Correlation: 1.0\n",
      "364: FEAT_A: 90d848e0 FEAT_B: Cauldron_Filler__Assessment__2000 - Correlation: 1.0\n",
      "365: FEAT_A: 795e4a37 FEAT_B: Cart_Balancer__Assessment__3010 - Correlation: 1.0\n",
      "366: FEAT_A: 28a4eb9a FEAT_B: 9de5e594 - Correlation: 0.9995923561196808\n",
      "367: FEAT_A: 28a4eb9a FEAT_B: Dino_Dive_3120 - Correlation: 0.9999999999999998\n",
      "368: FEAT_A: 28a4eb9a FEAT_B: Dino_Dive_3020 - Correlation: 0.9995923561196808\n",
      "369: FEAT_A: e79f3763 FEAT_B: Bug_Measurer__Activity__4030 - Correlation: 1.0\n",
      "370: FEAT_A: 05ad839b FEAT_B: Happy_Camel_4090 - Correlation: 1.0\n",
      "371: FEAT_A: d3640339 FEAT_B: Dino_Dive_4090 - Correlation: 1.0\n",
      "372: FEAT_A: 4c2ec19f FEAT_B: Egg_Dropper__Activity__4025 - Correlation: 1.0\n",
      "373: FEAT_A: 76babcde FEAT_B: Dino_Dive_4070 - Correlation: 1.0\n",
      "374: FEAT_A: 6f4bd64e FEAT_B: Air_Show_4090 - Correlation: 0.9999999999999998\n",
      "375: FEAT_A: 1b54d27f FEAT_B: Watering_Hole__Activity__2010 - Correlation: 1.0\n",
      "376: FEAT_A: 5f0eb72c FEAT_B: Mushroom_Sorter__Assessment__4020 - Correlation: 0.9999999999999998\n",
      "377: FEAT_A: c0415e5c FEAT_B: Dino_Dive_4020 - Correlation: 1.0\n",
      "378: FEAT_A: 4ef8cdd3 FEAT_B: Chow_Time_4020 - Correlation: 1.0\n",
      "379: FEAT_A: 8ac7cce4 FEAT_B: Leaf_Leader_2000 - Correlation: 1.0\n",
      "380: FEAT_A: 51311d7a FEAT_B: Dino_Drink_2000 - Correlation: 1.0\n",
      "381: FEAT_A: 160654fd FEAT_B: 88d4a5be - Correlation: 0.9989297172615879\n",
      "382: FEAT_A: 160654fd FEAT_B: Mushroom_Sorter__Assessment__3120 - Correlation: 0.9989297172615879\n",
      "383: FEAT_A: 160654fd FEAT_B: Mushroom_Sorter__Assessment__3020 - Correlation: 1.0\n",
      "384: FEAT_A: 01ca3a3c FEAT_B: Leaf_Leader_4080 - Correlation: 0.9999999999999999\n",
      "385: FEAT_A: 0086365d FEAT_B: Pan_Balance_4010 - Correlation: 0.9999999999999999\n",
      "386: FEAT_A: 587b5989 FEAT_B: All_Star_Sorting_4070 - Correlation: 1.0\n",
      "387: FEAT_A: bc8f2793 FEAT_B: Pan_Balance_4035 - Correlation: 0.9999999999999999\n",
      "388: FEAT_A: 19967db1 FEAT_B: Chow_Time_4090 - Correlation: 1.0\n",
      "389: FEAT_A: ec138c1c FEAT_B: Bird_Measurer__Assessment__2020 - Correlation: 0.9999999999999999\n",
      "390: FEAT_A: 1325467d FEAT_B: Sandcastle_Builder__Activity__4070 - Correlation: 1.0\n",
      "391: FEAT_A: 731c0cbe FEAT_B: Bird_Measurer__Assessment__4090 - Correlation: 1.0\n",
      "392: FEAT_A: fbaf3456 FEAT_B: Mushroom_Sorter__Assessment__4030 - Correlation: 0.9999999999999999\n",
      "393: FEAT_A: 5c3d2b2f FEAT_B: Scrub_A_Dub_4020 - Correlation: 1.0\n",
      "394: FEAT_A: 37db1c2f FEAT_B: Happy_Camel_4045 - Correlation: 1.0\n",
      "395: FEAT_A: 7ad3efc6 FEAT_B: 65a38bf7 - Correlation: 0.999978626569043\n",
      "396: FEAT_A: 7ad3efc6 FEAT_B: Cart_Balancer__Assessment__2000 - Correlation: 1.0\n",
      "397: FEAT_A: 7ad3efc6 FEAT_B: Cart_Balancer__Assessment__2020 - Correlation: 0.999978626569043\n",
      "398: FEAT_A: f54238ee FEAT_B: Fireworks__Activity__4090 - Correlation: 1.0\n",
      "399: FEAT_A: 3393b68b FEAT_B: Bird_Measurer__Assessment__2010 - Correlation: 1.0\n",
      "400: FEAT_A: ecc6157f FEAT_B: Cart_Balancer__Assessment__4080 - Correlation: 0.9999999999999999\n",
      "401: FEAT_A: 4b5efe37 FEAT_B: b7dc8128 - Correlation: 0.9980151285383981\n",
      "402: FEAT_A: 4b5efe37 FEAT_B: All_Star_Sorting_4010 - Correlation: 1.0\n",
      "403: FEAT_A: 4b5efe37 FEAT_B: All_Star_Sorting_2000 - Correlation: 0.9980151285383981\n",
      "404: FEAT_A: b1d5101d FEAT_B: All_Star_Sorting_4095 - Correlation: 1.0\n",
      "405: FEAT_A: 85d1b0de FEAT_B: Chicken_Balancer__Activity__4090 - Correlation: 1.0\n",
      "406: FEAT_A: b5053438 FEAT_B: d3268efa - Correlation: 0.999576326704631\n",
      "407: FEAT_A: b5053438 FEAT_B: 28520915 - Correlation: 0.9990905166101879\n",
      "408: FEAT_A: b5053438 FEAT_B: Cauldron_Filler__Assessment__3021 - Correlation: 0.999576326704631\n",
      "409: FEAT_A: b5053438 FEAT_B: Cauldron_Filler__Assessment__2030 - Correlation: 0.9990905166101879\n",
      "410: FEAT_A: b5053438 FEAT_B: Cauldron_Filler__Assessment__3121 - Correlation: 1.0\n",
      "411: FEAT_A: 022b4259 FEAT_B: Bug_Measurer__Activity__4025 - Correlation: 0.9999999999999998\n",
      "412: FEAT_A: 763fc34e FEAT_B: e57dd7af - Correlation: 0.9972721980394412\n",
      "413: FEAT_A: 763fc34e FEAT_B: Leaf_Leader_3120 - Correlation: 0.9972721980394412\n",
      "414: FEAT_A: 763fc34e FEAT_B: Leaf_Leader_3020 - Correlation: 1.0\n",
      "415: FEAT_A: bfc77bd6 FEAT_B: Chest_Sorter__Assessment__4080 - Correlation: 0.9999999999999998\n",
      "416: FEAT_A: 5d042115 FEAT_B: Flower_Waterer__Activity__4030 - Correlation: 1.0\n",
      "417: FEAT_A: d51b1749 FEAT_B: Happy_Camel_2080 - Correlation: 1.0\n",
      "418: FEAT_A: d122731b FEAT_B: Cart_Balancer__Assessment__4100 - Correlation: 0.9999999999999998\n",
      "419: FEAT_A: 67aa2ada FEAT_B: Leaf_Leader_4090 - Correlation: 1.0\n",
      "420: FEAT_A: 93b353f2 FEAT_B: Chest_Sorter__Assessment__4100 - Correlation: 0.9999999999999998\n",
      "421: FEAT_A: 4901243f FEAT_B: Fireworks__Activity__2000 - Correlation: 1.0\n",
      "422: FEAT_A: 9b23e8ee FEAT_B: 736f9581 - Correlation: 0.9999999999999999\n",
      "423: FEAT_A: 9b23e8ee FEAT_B: Egg_Dropper__Activity__2000 - Correlation: 0.9999999999999999\n",
      "424: FEAT_A: 9b23e8ee FEAT_B: Egg_Dropper__Activity__2020 - Correlation: 0.9999999999999999\n",
      "425: FEAT_A: 29bdd9ba FEAT_B: Dino_Dive_2000 - Correlation: 1.0\n",
      "426: FEAT_A: 8d84fa81 FEAT_B: Bubble_Bath_4010 - Correlation: 1.0\n",
      "427: FEAT_A: 565a3990 FEAT_B: Bug_Measurer__Activity__4070 - Correlation: 1.0\n",
      "428: FEAT_A: d02b7a8e FEAT_B: All_Star_Sorting_4035 - Correlation: 1.0\n",
      "429: FEAT_A: 7da34a02 FEAT_B: Mushroom_Sorter__Assessment__4070 - Correlation: 0.9999999999999998\n",
      "430: FEAT_A: 53c6e11a FEAT_B: Leaf_Leader_2075 - Correlation: 0.9999999999999999\n",
      "431: FEAT_A: 3afb49e6 FEAT_B: Chest_Sorter__Assessment__3021 - Correlation: 1.0\n",
      "432: FEAT_A: cfbd47c8 FEAT_B: Chow_Time_4030 - Correlation: 1.0\n",
      "433: FEAT_A: 77261ab5 FEAT_B: Sandcastle_Builder__Activity__2000 - Correlation: 0.9999999999999999\n",
      "434: FEAT_A: cc5087a3 FEAT_B: Crystals_Rule_4010 - Correlation: 1.0\n",
      "435: FEAT_A: e4d32835 FEAT_B: Pan_Balance_4080 - Correlation: 0.9999999999999998\n",
      "436: FEAT_A: 756e5507 FEAT_B: Chicken_Balancer__Activity__2000 - Correlation: 1.0\n",
      "437: FEAT_A: 84538528 FEAT_B: Sandcastle_Builder__Activity__4020 - Correlation: 1.0\n",
      "438: FEAT_A: 4e5fc6f5 FEAT_B: Cart_Balancer__Assessment__4090 - Correlation: 1.0\n",
      "439: FEAT_A: 46cd75b4 FEAT_B: Chicken_Balancer__Activity__4022 - Correlation: 0.9999999999999998\n",
      "440: FEAT_A: 5be391b5 FEAT_B: Dino_Drink_4010 - Correlation: 1.0\n",
      "441: FEAT_A: 93edfe2e FEAT_B: Crystals_Rule_4090 - Correlation: 1.0\n",
      "442: FEAT_A: 06372577 FEAT_B: Air_Show_2060 - Correlation: 1.0\n",
      "443: FEAT_A: 26a5a3dd FEAT_B: All_Star_Sorting_4080 - Correlation: 1.0\n",
      "444: FEAT_A: 6bf9e3e1 FEAT_B: Happy_Camel_4040 - Correlation: 0.9999999999999999\n",
      "445: FEAT_A: a7640a16 FEAT_B: Happy_Camel_4070 - Correlation: 1.0\n",
      "446: FEAT_A: 363c86c9 FEAT_B: Bug_Measurer__Activity__4035 - Correlation: 1.0\n",
      "447: FEAT_A: f50fc6c1 FEAT_B: Watering_Hole__Activity__4021 - Correlation: 1.0\n",
      "448: FEAT_A: 02a42007 FEAT_B: Fireworks__Activity__4030 - Correlation: 1.0\n",
      "449: FEAT_A: 5c2f29ca FEAT_B: Cart_Balancer__Assessment__4020 - Correlation: 1.0\n",
      "450: FEAT_A: d9c005dd FEAT_B: Happy_Camel_2000 - Correlation: 1.0\n",
      "451: FEAT_A: 3d8c61b0 FEAT_B: Happy_Camel_4030 - Correlation: 0.9999999999999999\n",
      "452: FEAT_A: 2a444e03 FEAT_B: Pan_Balance_4030 - Correlation: 1.0\n",
      "453: FEAT_A: 3d63345e FEAT_B: Cart_Balancer__Assessment__4035 - Correlation: 1.0\n",
      "454: FEAT_A: 1beb320a FEAT_B: Bubble_Bath_2020 - Correlation: 1.0\n",
      "455: FEAT_A: 3a4be871 FEAT_B: Flower_Waterer__Activity__4080 - Correlation: 1.0\n",
      "456: FEAT_A: 30df3273 FEAT_B: Sandcastle_Builder__Activity__4080 - Correlation: 1.0\n",
      "457: FEAT_A: d2659ab4 FEAT_B: Air_Show_2075 - Correlation: 1.0\n",
      "458: FEAT_A: beb0a7b9 FEAT_B: b88f38da - Correlation: 0.9999125179829754\n",
      "459: FEAT_A: beb0a7b9 FEAT_B: Fireworks__Activity__3110 - Correlation: 0.9999125179829754\n",
      "460: FEAT_A: beb0a7b9 FEAT_B: Fireworks__Activity__3010 - Correlation: 1.0\n",
      "461: FEAT_A: 070a5291 FEAT_B: Bird_Measurer__Assessment__4100 - Correlation: 1.0\n",
      "462: FEAT_A: c1cac9a2 FEAT_B: Scrub_A_Dub_2081 - Correlation: 0.9999999999999998\n",
      "463: FEAT_A: 5e812b27 FEAT_B: Sandcastle_Builder__Activity__4030 - Correlation: 1.0\n",
      "464: FEAT_A: 9ed8f6da FEAT_B: Dino_Drink_2075 - Correlation: 0.9999999999999999\n",
      "465: FEAT_A: 828e68f9 FEAT_B: Cart_Balancer__Assessment__3110 - Correlation: 1.0\n",
      "466: FEAT_A: 6043a2b4 FEAT_B: All_Star_Sorting_4090 - Correlation: 0.9999999999999999\n",
      "467: FEAT_A: 3ee399c3 FEAT_B: Cauldron_Filler__Assessment__4070 - Correlation: 1.0\n",
      "468: FEAT_A: 392e14df FEAT_B: Cauldron_Filler__Assessment__4100 - Correlation: 1.0\n",
      "469: FEAT_A: d3f1e122 FEAT_B: Bottle_Filler__Activity__4035 - Correlation: 1.0\n",
      "470: FEAT_A: d185d3ea FEAT_B: Chow_Time_4035 - Correlation: 1.0\n",
      "471: FEAT_A: 29f54413 FEAT_B: Leaf_Leader_2060 - Correlation: 0.9999999999999998\n",
      "472: FEAT_A: 6077cc36 FEAT_B: Bird_Measurer__Assessment__4080 - Correlation: 0.9999999999999998\n",
      "473: FEAT_A: 7d093bf9 FEAT_B: Chow_Time_2000 - Correlation: 0.9999999999999999\n",
      "474: FEAT_A: 13f56524 FEAT_B: Mushroom_Sorter__Assessment__4080 - Correlation: 1.0\n",
      "475: FEAT_A: 792530f8 FEAT_B: Dino_Drink_4030 - Correlation: 1.0\n",
      "476: FEAT_A: ab4ec3a4 FEAT_B: Dino_Drink_4080 - Correlation: 1.0\n",
      "477: FEAT_A: cdd22e43 FEAT_B: Chicken_Balancer__Activity__4035 - Correlation: 1.0\n",
      "478: FEAT_A: 0ce40006 FEAT_B: Happy_Camel_4080 - Correlation: 1.0\n",
      "479: FEAT_A: f6947f54 FEAT_B: Bird_Measurer__Assessment__2030 - Correlation: 1.0\n",
      "480: FEAT_A: 2ec694de FEAT_B: Bug_Measurer__Activity__4080 - Correlation: 0.9999999999999998\n",
      "481: FEAT_A: 5a848010 FEAT_B: Scrub_A_Dub_2080 - Correlation: 1.0\n",
      "482: FEAT_A: f32856e4 FEAT_B: Leaf_Leader_2020 - Correlation: 1.0\n",
      "483: FEAT_A: c952eb01 FEAT_B: Watering_Hole__Activity__4070 - Correlation: 1.0\n",
      "484: FEAT_A: 25fa8af4 FEAT_B: Mushroom_Sorter__Assessment__4100 - Correlation: 1.0\n",
      "485: FEAT_A: 9e34ea74 FEAT_B: Egg_Dropper__Activity__4070 - Correlation: 1.0\n",
      "486: FEAT_A: Lifting_Heavy_Things FEAT_B: Lifting_Heavy_Things_2000 - Correlation: 0.9999999999999999\n",
      "487: FEAT_A: Rulers FEAT_B: Rulers_2000 - Correlation: 1.0\n",
      "488: FEAT_A: Heavy__Heavier__Heaviest FEAT_B: Heavy__Heavier__Heaviest_2000 - Correlation: 1.0\n",
      "489: FEAT_A: Costume_Box FEAT_B: Costume_Box_2000 - Correlation: 1.0\n",
      "490: FEAT_A: Crystal_Caves___Level_2 FEAT_B: Crystal_Caves___Level_2_2000 - Correlation: 1.0\n",
      "491: FEAT_A: Magma_Peak___Level_2 FEAT_B: Magma_Peak___Level_2_2000 - Correlation: 1.0\n",
      "492: FEAT_A: Tree_Top_City___Level_3 FEAT_B: Tree_Top_City___Level_3_2000 - Correlation: 1.0\n",
      "493: FEAT_A: Slop_Problem FEAT_B: Slop_Problem_2000 - Correlation: 1.0\n",
      "494: FEAT_A: Ordering_Spheres FEAT_B: Ordering_Spheres_2000 - Correlation: 1.0\n",
      "495: FEAT_A: Crystal_Caves___Level_3 FEAT_B: Crystal_Caves___Level_3_2000 - Correlation: 1.0\n",
      "496: FEAT_A: Welcome_to_Lost_Lagoon_ FEAT_B: Welcome_to_Lost_Lagoon__2000 - Correlation: 1.0\n",
      "497: FEAT_A: 12_Monkeys FEAT_B: 12_Monkeys_2000 - Correlation: 1.0\n",
      "498: FEAT_A: Crystal_Caves___Level_1 FEAT_B: Crystal_Caves___Level_1_2000 - Correlation: 1.0\n",
      "499: FEAT_A: Treasure_Map FEAT_B: Treasure_Map_2000 - Correlation: 1.0\n",
      "500: FEAT_A: Balancing_Act FEAT_B: Balancing_Act_2000 - Correlation: 1.0\n",
      "501: FEAT_A: Honey_Cake FEAT_B: Honey_Cake_2000 - Correlation: 1.0\n",
      "502: FEAT_A: Pirate_s_Tale FEAT_B: Pirate_s_Tale_2000 - Correlation: 0.9999999999999999\n",
      "503: FEAT_A: Magma_Peak___Level_1 FEAT_B: Magma_Peak___Level_1_2000 - Correlation: 1.0\n",
      "504: FEAT_A: Tree_Top_City___Level_2 FEAT_B: Tree_Top_City___Level_2_2000 - Correlation: 1.0\n",
      "505: FEAT_A: Tree_Top_City___Level_1 FEAT_B: Tree_Top_City___Level_1_2000 - Correlation: 0.9999999999999999\n",
      "506: FEAT_A: var_event_id FEAT_B: var_title_event_code - Correlation: 0.9995254184300616\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "to_remove = []\n",
    "for feat_a in features:\n",
    "    for feat_b in features:\n",
    "        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "            if c > 0.995:\n",
    "                counter += 1\n",
    "                to_remove.append(feat_b)\n",
    "                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_Cart_Balancer__Assessment_ -0.04020325710970116 -0.47065833333333346 0.006732930476733109\n",
      "29a42aea 0.004070096099491238 0.0\n",
      "611485c5 0.0013566986998304127 0.0\n",
      "119b5b02 0.0002826455624646693 0.0\n",
      "6aeafed4 0.14703222159412097 0.008 0.0\n",
      "003cd2ee 0.0 0.0\n",
      "17ca3959 0.0 0.0\n",
      "eb2c19cd 0.17382702091577162 0.008 0.0\n",
      "7fd1ac25 0.01978518937252685 0.0\n",
      "1b54d27f 0.0007348784624081402 0.0\n",
      "01ca3a3c 0.0004522328999434709 0.0\n",
      "dcb1663e 0.0 0.0\n",
      "5dc079d8 0.0 0.0\n",
      "ecc6157f 0.007292255511588468 0.0\n",
      "bfc77bd6 0.012832108535895986 0.0\n",
      "a8cc6fec 0.0 0.0\n",
      "e4d32835 0.0013001695873374789 0.0\n",
      "4074bac2 0.0 0.0\n",
      "13f56524 0.04392312040700961 0.0\n",
      "ab4ec3a4 0.0009044657998869418 0.0\n",
      "0ce40006 0.0008479366873940079 0.0\n",
      "2ec694de 0.008988128886376484 0.0\n",
      "Mushroom_Sorter__Assessment__4080 0.04392312040700961 0.0\n",
      "Bubble_Bath_4080 0.004070096099491238 0.0\n",
      "Air_Show_4080 0.0 0.0\n",
      "Pan_Balance_2010 0.0 0.0\n",
      "Chest_Sorter__Assessment__4080 0.012832108535895986 0.0\n",
      "Fireworks__Activity__4080 0.0013566986998304127 0.0\n",
      "Happy_Camel_4080 0.0008479366873940079 0.0\n",
      "Mushroom_Sorter__Assessment__4090 0.17382702091577162 0.008 0.0\n",
      "Dino_Drink_4080 0.0009044657998869418 0.0\n",
      "Bottle_Filler__Activity__2010 0.0 0.0\n",
      "Dino_Dive_4080 0.0002826455624646693 0.0\n",
      "Sandcastle_Builder__Activity__2010 0.0 0.0\n",
      "Egg_Dropper__Activity__4080 0.01978518937252685 0.0\n",
      "Scrub_A_Dub_4080 0.0 0.0\n",
      "Crystals_Rule_2010 0.0 0.0\n",
      "Bug_Measurer__Activity__4080 0.008988128886376484 0.0\n",
      "Watering_Hole__Activity__2010 0.0007348784624081402 0.0\n",
      "Pan_Balance_4080 0.0013001695873374789 0.0\n",
      "Leaf_Leader_4080 0.0004522328999434709 0.0\n",
      "Bubble_Bath_4090 0.14703222159412097 0.008 0.0\n",
      "Cart_Balancer__Assessment__4080 0.007292255511588468 0.0\n"
     ]
    }
   ],
   "source": [
    "to_exclude = [] \n",
    "ajusted_test = reduce_test.copy()\n",
    "for feature in ajusted_test.columns:\n",
    "    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title']:\n",
    "        data = reduce_train[feature]\n",
    "        train_mean = data.mean()\n",
    "        data = ajusted_test[feature] \n",
    "        test_mean = data.mean()\n",
    "        try:\n",
    "            error = stract_hists(feature, adjust=True)\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature, train_mean, test_mean, error)\n",
    "            else:\n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "        except:\n",
    "            to_exclude.append(feature)\n",
    "            print(feature, train_mean, test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17690, 365)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [x for x in features if x not in (to_exclude + to_remove)]\n",
    "reduce_train[features].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2470400\ttest: 1.2474611\tbest: 1.2474611 (0)\ttotal: 110ms\tremaining: 9m 9s\n",
      "100:\tlearn: 1.0154422\ttest: 1.0220335\tbest: 1.0220335 (100)\ttotal: 4.48s\tremaining: 3m 37s\n",
      "200:\tlearn: 0.9897011\ttest: 1.0040697\tbest: 1.0040697 (200)\ttotal: 8.83s\tremaining: 3m 30s\n",
      "300:\tlearn: 0.9750721\ttest: 0.9976290\tbest: 0.9976290 (300)\ttotal: 13.2s\tremaining: 3m 25s\n",
      "400:\tlearn: 0.9621608\ttest: 0.9922792\tbest: 0.9922792 (400)\ttotal: 17.4s\tremaining: 3m 19s\n",
      "500:\tlearn: 0.9515259\ttest: 0.9894668\tbest: 0.9894668 (500)\ttotal: 21.5s\tremaining: 3m 13s\n",
      "600:\tlearn: 0.9412770\ttest: 0.9868688\tbest: 0.9868688 (600)\ttotal: 25.7s\tremaining: 3m 8s\n",
      "700:\tlearn: 0.9305941\ttest: 0.9852487\tbest: 0.9852487 (700)\ttotal: 30s\tremaining: 3m 3s\n",
      "800:\tlearn: 0.9209055\ttest: 0.9838658\tbest: 0.9838503 (797)\ttotal: 34.3s\tremaining: 2m 59s\n",
      "900:\tlearn: 0.9112280\ttest: 0.9829827\tbest: 0.9829649 (894)\ttotal: 38.7s\tremaining: 2m 55s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.9828587533\n",
      "bestIteration = 907\n",
      "\n",
      "Shrink model to first 908 iterations.\n",
      "Partial score of fold 0 is: 0.5922656060107878\n",
      "0:\tlearn: 1.2472543\ttest: 1.2470450\tbest: 1.2470450 (0)\ttotal: 45.1ms\tremaining: 3m 45s\n",
      "100:\tlearn: 1.0178727\ttest: 1.0179783\tbest: 1.0179783 (100)\ttotal: 4.46s\tremaining: 3m 36s\n",
      "200:\tlearn: 0.9893453\ttest: 1.0041432\tbest: 1.0041432 (200)\ttotal: 8.9s\tremaining: 3m 32s\n",
      "300:\tlearn: 0.9749346\ttest: 1.0003897\tbest: 1.0003897 (300)\ttotal: 13.2s\tremaining: 3m 26s\n",
      "400:\tlearn: 0.9610280\ttest: 0.9978564\tbest: 0.9978564 (400)\ttotal: 17.4s\tremaining: 3m 19s\n",
      "500:\tlearn: 0.9492281\ttest: 0.9947657\tbest: 0.9947657 (500)\ttotal: 21.7s\tremaining: 3m 14s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.9938182057\n",
      "bestIteration = 545\n",
      "\n",
      "Shrink model to first 546 iterations.\n",
      "Partial score of fold 1 is: 0.5809355418772589\n",
      "0:\tlearn: 1.2472762\ttest: 1.2473258\tbest: 1.2473258 (0)\ttotal: 45ms\tremaining: 3m 44s\n",
      "100:\tlearn: 1.0147035\ttest: 1.0283052\tbest: 1.0283052 (100)\ttotal: 4.43s\tremaining: 3m 35s\n",
      "200:\tlearn: 0.9881798\ttest: 1.0110634\tbest: 1.0110634 (200)\ttotal: 8.84s\tremaining: 3m 30s\n",
      "300:\tlearn: 0.9729068\ttest: 1.0045197\tbest: 1.0045197 (300)\ttotal: 13.2s\tremaining: 3m 25s\n",
      "400:\tlearn: 0.9592527\ttest: 1.0011906\tbest: 1.0011906 (400)\ttotal: 17.5s\tremaining: 3m 20s\n",
      "500:\tlearn: 0.9461920\ttest: 0.9979284\tbest: 0.9979183 (498)\ttotal: 21.7s\tremaining: 3m 14s\n",
      "600:\tlearn: 0.9359487\ttest: 0.9965295\tbest: 0.9965295 (600)\ttotal: 25.8s\tremaining: 3m 9s\n",
      "700:\tlearn: 0.9276274\ttest: 0.9951054\tbest: 0.9951054 (700)\ttotal: 29.9s\tremaining: 3m 3s\n",
      "800:\tlearn: 0.9204071\ttest: 0.9943577\tbest: 0.9943539 (798)\ttotal: 34s\tremaining: 2m 58s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.9940650762\n",
      "bestIteration = 855\n",
      "\n",
      "Shrink model to first 856 iterations.\n",
      "Partial score of fold 2 is: 0.5830827551554284\n",
      "0:\tlearn: 1.2471047\ttest: 1.2470745\tbest: 1.2470745 (0)\ttotal: 43.3ms\tremaining: 3m 36s\n",
      "100:\tlearn: 1.0143061\ttest: 1.0222785\tbest: 1.0222785 (100)\ttotal: 4.43s\tremaining: 3m 34s\n",
      "200:\tlearn: 0.9875787\ttest: 1.0068848\tbest: 1.0068848 (200)\ttotal: 8.89s\tremaining: 3m 32s\n",
      "300:\tlearn: 0.9736413\ttest: 1.0016565\tbest: 1.0016363 (299)\ttotal: 13.2s\tremaining: 3m 26s\n",
      "400:\tlearn: 0.9604130\ttest: 0.9979158\tbest: 0.9979063 (399)\ttotal: 17.4s\tremaining: 3m 19s\n",
      "500:\tlearn: 0.9472673\ttest: 0.9952060\tbest: 0.9951651 (498)\ttotal: 21.7s\tremaining: 3m 14s\n",
      "600:\tlearn: 0.9360424\ttest: 0.9925201\tbest: 0.9925201 (600)\ttotal: 25.9s\tremaining: 3m 9s\n",
      "700:\tlearn: 0.9259132\ttest: 0.9912379\tbest: 0.9912107 (692)\ttotal: 30.1s\tremaining: 3m 4s\n",
      "800:\tlearn: 0.9170195\ttest: 0.9899972\tbest: 0.9899972 (800)\ttotal: 34.3s\tremaining: 2m 59s\n",
      "900:\tlearn: 0.9087811\ttest: 0.9890269\tbest: 0.9890269 (900)\ttotal: 38.5s\tremaining: 2m 55s\n",
      "1000:\tlearn: 0.9022801\ttest: 0.9880281\tbest: 0.9880281 (1000)\ttotal: 42.6s\tremaining: 2m 50s\n",
      "1100:\tlearn: 0.8944162\ttest: 0.9874847\tbest: 0.9874847 (1100)\ttotal: 46.9s\tremaining: 2m 45s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.9874202289\n",
      "bestIteration = 1103\n",
      "\n",
      "Shrink model to first 1104 iterations.\n",
      "Partial score of fold 3 is: 0.5897033294297842\n",
      "0:\tlearn: 1.2466030\ttest: 1.2467881\tbest: 1.2467881 (0)\ttotal: 44.9ms\tremaining: 3m 44s\n",
      "100:\tlearn: 1.0147708\ttest: 1.0308451\tbest: 1.0308451 (100)\ttotal: 4.44s\tremaining: 3m 35s\n",
      "200:\tlearn: 0.9895963\ttest: 1.0137331\tbest: 1.0137331 (200)\ttotal: 8.83s\tremaining: 3m 30s\n",
      "300:\tlearn: 0.9736622\ttest: 1.0057785\tbest: 1.0057785 (300)\ttotal: 13.2s\tremaining: 3m 26s\n",
      "400:\tlearn: 0.9602017\ttest: 1.0005162\tbest: 1.0005162 (400)\ttotal: 17.5s\tremaining: 3m 20s\n",
      "500:\tlearn: 0.9484182\ttest: 0.9964138\tbest: 0.9964138 (500)\ttotal: 21.6s\tremaining: 3m 14s\n",
      "600:\tlearn: 0.9374945\ttest: 0.9933170\tbest: 0.9933170 (600)\ttotal: 25.8s\tremaining: 3m 9s\n",
      "700:\tlearn: 0.9273521\ttest: 0.9917283\tbest: 0.9917103 (698)\ttotal: 30.1s\tremaining: 3m 4s\n",
      "800:\tlearn: 0.9196735\ttest: 0.9903757\tbest: 0.9903757 (800)\ttotal: 34.8s\tremaining: 3m 2s\n",
      "900:\tlearn: 0.9123209\ttest: 0.9893245\tbest: 0.9893245 (900)\ttotal: 39s\tremaining: 2m 57s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.9890292738\n",
      "bestIteration = 932\n",
      "\n",
      "Shrink model to first 933 iterations.\n",
      "Partial score of fold 4 is: 0.5803081195115964\n",
      "Our oof cohen kappa score is:  0.5846161848072026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0367\tvalid_1's rmse: 1.05497\n",
      "[200]\ttraining's rmse: 0.96786\tvalid_1's rmse: 1.0048\n",
      "[300]\ttraining's rmse: 0.933817\tvalid_1's rmse: 0.98812\n",
      "[400]\ttraining's rmse: 0.90986\tvalid_1's rmse: 0.981031\n",
      "[500]\ttraining's rmse: 0.890716\tvalid_1's rmse: 0.978121\n",
      "[600]\ttraining's rmse: 0.874093\tvalid_1's rmse: 0.975865\n",
      "[700]\ttraining's rmse: 0.859168\tvalid_1's rmse: 0.975028\n",
      "[800]\ttraining's rmse: 0.845327\tvalid_1's rmse: 0.974685\n",
      "[900]\ttraining's rmse: 0.832272\tvalid_1's rmse: 0.974566\n",
      "Early stopping, best iteration is:\n",
      "[878]\ttraining's rmse: 0.835172\tvalid_1's rmse: 0.974489\n",
      "Partial score of fold 0 is: 0.6006743451584371\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03512\tvalid_1's rmse: 1.05471\n",
      "[200]\ttraining's rmse: 0.965794\tvalid_1's rmse: 1.00794\n",
      "[300]\ttraining's rmse: 0.931055\tvalid_1's rmse: 0.99522\n",
      "[400]\ttraining's rmse: 0.906741\tvalid_1's rmse: 0.990804\n",
      "[500]\ttraining's rmse: 0.887413\tvalid_1's rmse: 0.989131\n",
      "[600]\ttraining's rmse: 0.870762\tvalid_1's rmse: 0.988721\n",
      "Early stopping, best iteration is:\n",
      "[592]\ttraining's rmse: 0.872004\tvalid_1's rmse: 0.988705\n",
      "Partial score of fold 1 is: 0.5855878373132928\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03464\tvalid_1's rmse: 1.05632\n",
      "[200]\ttraining's rmse: 0.96554\tvalid_1's rmse: 1.00758\n",
      "[300]\ttraining's rmse: 0.93172\tvalid_1's rmse: 0.993181\n",
      "[400]\ttraining's rmse: 0.907979\tvalid_1's rmse: 0.987231\n",
      "[500]\ttraining's rmse: 0.888597\tvalid_1's rmse: 0.98492\n",
      "[600]\ttraining's rmse: 0.872204\tvalid_1's rmse: 0.983732\n",
      "[700]\ttraining's rmse: 0.857428\tvalid_1's rmse: 0.982679\n",
      "[800]\ttraining's rmse: 0.843825\tvalid_1's rmse: 0.98239\n",
      "Early stopping, best iteration is:\n",
      "[741]\ttraining's rmse: 0.851843\tvalid_1's rmse: 0.9823\n",
      "Partial score of fold 2 is: 0.5916716082681062\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0311\tvalid_1's rmse: 1.053\n",
      "[200]\ttraining's rmse: 0.964951\tvalid_1's rmse: 1.00855\n",
      "[300]\ttraining's rmse: 0.931074\tvalid_1's rmse: 0.994701\n",
      "[400]\ttraining's rmse: 0.907796\tvalid_1's rmse: 0.989248\n",
      "[500]\ttraining's rmse: 0.88856\tvalid_1's rmse: 0.986645\n",
      "[600]\ttraining's rmse: 0.871936\tvalid_1's rmse: 0.985496\n",
      "[700]\ttraining's rmse: 0.85694\tvalid_1's rmse: 0.984917\n",
      "[800]\ttraining's rmse: 0.843116\tvalid_1's rmse: 0.984626\n",
      "[900]\ttraining's rmse: 0.830363\tvalid_1's rmse: 0.984236\n",
      "[1000]\ttraining's rmse: 0.81809\tvalid_1's rmse: 0.983841\n",
      "[1100]\ttraining's rmse: 0.806603\tvalid_1's rmse: 0.984158\n",
      "Early stopping, best iteration is:\n",
      "[1000]\ttraining's rmse: 0.81809\tvalid_1's rmse: 0.983841\n",
      "Partial score of fold 3 is: 0.5966817725838349\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03052\tvalid_1's rmse: 1.05491\n",
      "[200]\ttraining's rmse: 0.965138\tvalid_1's rmse: 1.01056\n",
      "[300]\ttraining's rmse: 0.93184\tvalid_1's rmse: 0.995033\n",
      "[400]\ttraining's rmse: 0.908313\tvalid_1's rmse: 0.98818\n",
      "[500]\ttraining's rmse: 0.889426\tvalid_1's rmse: 0.984744\n",
      "[600]\ttraining's rmse: 0.873251\tvalid_1's rmse: 0.982689\n",
      "[700]\ttraining's rmse: 0.85818\tvalid_1's rmse: 0.981497\n",
      "[800]\ttraining's rmse: 0.84446\tvalid_1's rmse: 0.980549\n",
      "[900]\ttraining's rmse: 0.831563\tvalid_1's rmse: 0.979959\n",
      "[1000]\ttraining's rmse: 0.819339\tvalid_1's rmse: 0.97965\n",
      "[1100]\ttraining's rmse: 0.807848\tvalid_1's rmse: 0.97971\n",
      "Early stopping, best iteration is:\n",
      "[1004]\ttraining's rmse: 0.818805\tvalid_1's rmse: 0.979619\n",
      "Partial score of fold 4 is: 0.5881846146028695\n",
      "Our oof cohen kappa score is:  0.5931700993196167\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_model = Catb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)\n",
    "lgb_model = Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.80266\tval-rmse:1.80449\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.776581\tval-rmse:0.984486\n",
      "[200]\ttrain-rmse:0.742573\tval-rmse:0.978951\n",
      "[300]\ttrain-rmse:0.731936\tval-rmse:0.978403\n",
      "Stopping. Best iteration:\n",
      "[289]\ttrain-rmse:0.732496\tval-rmse:0.978295\n",
      "\n",
      "Partial score of fold 0 is: 0.597275067630664\n",
      "[0]\ttrain-rmse:1.8025\tval-rmse:1.80578\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.772715\tval-rmse:0.992982\n",
      "[200]\ttrain-rmse:0.735113\tval-rmse:0.989597\n",
      "Stopping. Best iteration:\n",
      "[187]\ttrain-rmse:0.737085\tval-rmse:0.989369\n",
      "\n",
      "Partial score of fold 1 is: 0.5877350505914622\n",
      "[0]\ttrain-rmse:1.80236\tval-rmse:1.80523\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.776299\tval-rmse:0.986817\n",
      "[200]\ttrain-rmse:0.741347\tval-rmse:0.985289\n",
      "Stopping. Best iteration:\n",
      "[193]\ttrain-rmse:0.741547\tval-rmse:0.985205\n",
      "\n",
      "Partial score of fold 2 is: 0.5884507883508521\n",
      "[0]\ttrain-rmse:1.80262\tval-rmse:1.80505\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.773478\tval-rmse:0.991842\n",
      "[200]\ttrain-rmse:0.737342\tval-rmse:0.988403\n",
      "Stopping. Best iteration:\n",
      "[174]\ttrain-rmse:0.74272\tval-rmse:0.988006\n",
      "\n",
      "Partial score of fold 3 is: 0.5972185759033773\n",
      "[0]\ttrain-rmse:1.80236\tval-rmse:1.80559\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.77475\tval-rmse:0.989211\n",
      "[200]\ttrain-rmse:0.73824\tval-rmse:0.985388\n",
      "[300]\ttrain-rmse:0.727394\tval-rmse:0.9851\n",
      "[400]\ttrain-rmse:0.724235\tval-rmse:0.984444\n",
      "[500]\ttrain-rmse:0.720257\tval-rmse:0.984138\n",
      "[600]\ttrain-rmse:0.716955\tval-rmse:0.984214\n",
      "Stopping. Best iteration:\n",
      "[551]\ttrain-rmse:0.719175\tval-rmse:0.98406\n",
      "\n",
      "Partial score of fold 4 is: 0.5871105470904232\n",
      "Our oof cohen kappa score is:  0.591881643577161\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model = Xgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:100: DeprecationWarning: The ``active_features_`` attribute was deprecated in version 0.20 and will be removed 0.22.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 370)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 370)]             0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 50, 370)           0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 370, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14151 samples, validate on 3539 samples\n",
      "Epoch 1/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.7210\n",
      "Epoch 00001: val_loss improved from inf to 1.07696, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 6s 434us/sample - loss: 1.7199 - val_loss: 1.0770\n",
      "Epoch 2/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.3902\n",
      "Epoch 00002: val_loss improved from 1.07696 to 1.07439, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.3923 - val_loss: 1.0744\n",
      "Epoch 3/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.2933\n",
      "Epoch 00003: val_loss did not improve from 1.07439\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 1.2910 - val_loss: 1.0856\n",
      "Epoch 4/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.2443\n",
      "Epoch 00004: val_loss improved from 1.07439 to 1.03857, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 1.2440 - val_loss: 1.0386\n",
      "Epoch 5/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.2112\n",
      "Epoch 00005: val_loss improved from 1.03857 to 1.02813, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 362us/sample - loss: 1.2111 - val_loss: 1.0281\n",
      "Epoch 6/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.1943\n",
      "Epoch 00006: val_loss did not improve from 1.02813\n",
      "14151/14151 [==============================] - 5s 362us/sample - loss: 1.1945 - val_loss: 1.0409\n",
      "Epoch 7/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.1730\n",
      "Epoch 00007: val_loss improved from 1.02813 to 1.02393, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.1718 - val_loss: 1.0239\n",
      "Epoch 8/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.1513\n",
      "Epoch 00008: val_loss did not improve from 1.02393\n",
      "14151/14151 [==============================] - 5s 358us/sample - loss: 1.1507 - val_loss: 1.0285\n",
      "Epoch 9/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.1278\n",
      "Epoch 00009: val_loss improved from 1.02393 to 1.00149, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 6s 391us/sample - loss: 1.1272 - val_loss: 1.0015\n",
      "Epoch 10/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.1130\n",
      "Epoch 00010: val_loss did not improve from 1.00149\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.1128 - val_loss: 1.0154\n",
      "Epoch 11/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.0927\n",
      "Epoch 00011: val_loss did not improve from 1.00149\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.0924 - val_loss: 1.0017\n",
      "Epoch 12/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0904\n",
      "Epoch 00012: val_loss improved from 1.00149 to 0.99977, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.0915 - val_loss: 0.9998\n",
      "Epoch 13/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0777\n",
      "Epoch 00013: val_loss improved from 0.99977 to 0.99036, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 1.0776 - val_loss: 0.9904\n",
      "Epoch 14/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0791\n",
      "Epoch 00014: val_loss did not improve from 0.99036\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 1.0790 - val_loss: 0.9912\n",
      "Epoch 15/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0614\n",
      "Epoch 00015: val_loss did not improve from 0.99036\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.0603 - val_loss: 0.9933\n",
      "Epoch 16/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0518\n",
      "Epoch 00016: val_loss did not improve from 0.99036\n",
      "14151/14151 [==============================] - 5s 356us/sample - loss: 1.0536 - val_loss: 0.9919\n",
      "Epoch 17/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 1.0390\n",
      "Epoch 00017: val_loss improved from 0.99036 to 0.98778, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 358us/sample - loss: 1.0401 - val_loss: 0.9878\n",
      "Epoch 18/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 1.0318\n",
      "Epoch 00018: val_loss did not improve from 0.98778\n",
      "14151/14151 [==============================] - 5s 360us/sample - loss: 1.0317 - val_loss: 1.0139\n",
      "Epoch 19/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 1.0320\n",
      "Epoch 00019: val_loss did not improve from 0.98778\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 1.0340 - val_loss: 0.9883\n",
      "Epoch 20/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 1.0161\n",
      "Epoch 00020: val_loss improved from 0.98778 to 0.98063, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 358us/sample - loss: 1.0164 - val_loss: 0.9806\n",
      "Epoch 21/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 1.0017\n",
      "Epoch 00021: val_loss improved from 0.98063 to 0.97900, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 1.0012 - val_loss: 0.9790\n",
      "Epoch 22/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 1.0026\n",
      "Epoch 00022: val_loss did not improve from 0.97900\n",
      "14151/14151 [==============================] - 5s 356us/sample - loss: 1.0010 - val_loss: 0.9838\n",
      "Epoch 23/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9952\n",
      "Epoch 00023: val_loss did not improve from 0.97900\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 0.9953 - val_loss: 0.9807\n",
      "Epoch 24/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9865\n",
      "Epoch 00024: val_loss improved from 0.97900 to 0.96226, saving model to nn_model.w8\n",
      "14151/14151 [==============================] - 5s 360us/sample - loss: 0.9864 - val_loss: 0.9623\n",
      "Epoch 25/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.9669\n",
      "Epoch 00025: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 355us/sample - loss: 0.9672 - val_loss: 0.9874\n",
      "Epoch 26/100\n",
      "14016/14151 [============================>.] - ETA: 0s - loss: 0.9607\n",
      "Epoch 00026: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 0.9607 - val_loss: 0.9759\n",
      "Epoch 27/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9554\n",
      "Epoch 00027: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 0.9553 - val_loss: 0.9829\n",
      "Epoch 28/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.9460\n",
      "Epoch 00028: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 355us/sample - loss: 0.9455 - val_loss: 0.9804\n",
      "Epoch 29/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9327\n",
      "Epoch 00029: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 0.9327 - val_loss: 0.9858\n",
      "Epoch 30/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9304\n",
      "Epoch 00030: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 358us/sample - loss: 0.9303 - val_loss: 1.0029\n",
      "Epoch 31/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.9183\n",
      "Epoch 00031: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 358us/sample - loss: 0.9183 - val_loss: 0.9702\n",
      "Epoch 32/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.9157\n",
      "Epoch 00032: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 0.9163 - val_loss: 0.9981\n",
      "Epoch 33/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8960\n",
      "Epoch 00033: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 0.8950 - val_loss: 0.9842\n",
      "Epoch 34/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8961\n",
      "Epoch 00034: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 358us/sample - loss: 0.8960 - val_loss: 1.0104\n",
      "Epoch 35/100\n",
      "14112/14151 [============================>.] - ETA: 0s - loss: 0.8902\n",
      "Epoch 00035: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 356us/sample - loss: 0.8903 - val_loss: 0.9789\n",
      "Epoch 36/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8834\n",
      "Epoch 00036: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 0.8835 - val_loss: 1.0040\n",
      "Epoch 37/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8750\n",
      "Epoch 00037: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 363us/sample - loss: 0.8752 - val_loss: 1.0070\n",
      "Epoch 38/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8653\n",
      "Epoch 00038: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 380us/sample - loss: 0.8651 - val_loss: 1.0030\n",
      "Epoch 39/100\n",
      "13984/14151 [============================>.] - ETA: 0s - loss: 0.8625\n",
      "Epoch 00039: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 369us/sample - loss: 0.8630 - val_loss: 1.0194\n",
      "Epoch 40/100\n",
      "14048/14151 [============================>.] - ETA: 0s - loss: 0.8429\n",
      "Epoch 00040: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 6s 403us/sample - loss: 0.8434 - val_loss: 0.9997\n",
      "Epoch 41/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8418\n",
      "Epoch 00041: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 376us/sample - loss: 0.8408 - val_loss: 1.0136\n",
      "Epoch 42/100\n",
      "14080/14151 [============================>.] - ETA: 0s - loss: 0.8394\n",
      "Epoch 00042: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 359us/sample - loss: 0.8397 - val_loss: 1.0122\n",
      "Epoch 43/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8255\n",
      "Epoch 00043: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 357us/sample - loss: 0.8251 - val_loss: 1.0272\n",
      "Epoch 44/100\n",
      "14144/14151 [============================>.] - ETA: 0s - loss: 0.8240\n",
      "Epoch 00044: val_loss did not improve from 0.96226\n",
      "14151/14151 [==============================] - 5s 362us/sample - loss: 0.8241 - val_loss: 1.0076\n",
      "Partial score of fold 0 is: 0.6033579853119423\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 370)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 50, 370)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 50, 370, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.7702\n",
      "Epoch 00001: val_loss improved from inf to 1.09799, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 413us/sample - loss: 1.7706 - val_loss: 1.0980\n",
      "Epoch 2/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.4298\n",
      "Epoch 00002: val_loss improved from 1.09799 to 1.04523, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 362us/sample - loss: 1.4313 - val_loss: 1.0452\n",
      "Epoch 3/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.3261\n",
      "Epoch 00003: val_loss improved from 1.04523 to 1.03548, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 1.3247 - val_loss: 1.0355\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.2598\n",
      "Epoch 00004: val_loss improved from 1.03548 to 1.02790, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 364us/sample - loss: 1.2599 - val_loss: 1.0279\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2286\n",
      "Epoch 00005: val_loss did not improve from 1.02790\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.2284 - val_loss: 1.0443\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1912\n",
      "Epoch 00006: val_loss improved from 1.02790 to 1.02686, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 1.1914 - val_loss: 1.0269\n",
      "Epoch 7/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1631\n",
      "Epoch 00007: val_loss did not improve from 1.02686\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 1.1641 - val_loss: 1.0658\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1504\n",
      "Epoch 00008: val_loss improved from 1.02686 to 1.02200, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.1500 - val_loss: 1.0220\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1319\n",
      "Epoch 00009: val_loss did not improve from 1.02200\n",
      "14152/14152 [==============================] - 5s 367us/sample - loss: 1.1318 - val_loss: 1.0349\n",
      "Epoch 10/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1082\n",
      "Epoch 00010: val_loss improved from 1.02200 to 1.00824, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 365us/sample - loss: 1.1084 - val_loss: 1.0082\n",
      "Epoch 11/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0930\n",
      "Epoch 00011: val_loss did not improve from 1.00824\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 1.0920 - val_loss: 1.0192\n",
      "Epoch 12/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0839\n",
      "Epoch 00012: val_loss did not improve from 1.00824\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 1.0843 - val_loss: 1.0174\n",
      "Epoch 13/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0605\n",
      "Epoch 00013: val_loss did not improve from 1.00824\n",
      "14152/14152 [==============================] - 5s 353us/sample - loss: 1.0607 - val_loss: 1.0249\n",
      "Epoch 14/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0501\n",
      "Epoch 00014: val_loss improved from 1.00824 to 1.00036, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.0498 - val_loss: 1.0004\n",
      "Epoch 15/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0382\n",
      "Epoch 00015: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.0378 - val_loss: 1.0158\n",
      "Epoch 16/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0318\n",
      "Epoch 00016: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.0318 - val_loss: 1.0133\n",
      "Epoch 17/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0218\n",
      "Epoch 00017: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 354us/sample - loss: 1.0216 - val_loss: 1.0111\n",
      "Epoch 18/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0056\n",
      "Epoch 00018: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 1.0055 - val_loss: 1.0091\n",
      "Epoch 19/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0002\n",
      "Epoch 00019: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.0001 - val_loss: 1.0295\n",
      "Epoch 20/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9787\n",
      "Epoch 00020: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 0.9779 - val_loss: 1.0358\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9749\n",
      "Epoch 00021: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 0.9734 - val_loss: 1.0260\n",
      "Epoch 22/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9598\n",
      "Epoch 00022: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 0.9595 - val_loss: 1.0224\n",
      "Epoch 23/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9432\n",
      "Epoch 00023: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 7s 510us/sample - loss: 0.9427 - val_loss: 1.0073\n",
      "Epoch 24/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9392\n",
      "Epoch 00024: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 0.9395 - val_loss: 1.0100\n",
      "Epoch 25/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9333\n",
      "Epoch 00025: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 0.9342 - val_loss: 1.0126\n",
      "Epoch 26/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9234\n",
      "Epoch 00026: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.9225 - val_loss: 1.0063\n",
      "Epoch 27/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9142\n",
      "Epoch 00027: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 0.9151 - val_loss: 1.0091\n",
      "Epoch 28/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9086\n",
      "Epoch 00028: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 0.9087 - val_loss: 1.0229\n",
      "Epoch 29/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8946\n",
      "Epoch 00029: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.8957 - val_loss: 1.0494\n",
      "Epoch 30/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8899\n",
      "Epoch 00030: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.8898 - val_loss: 1.0465\n",
      "Epoch 31/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8847\n",
      "Epoch 00031: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.8851 - val_loss: 1.0274\n",
      "Epoch 32/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8753\n",
      "Epoch 00032: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.8768 - val_loss: 1.0264\n",
      "Epoch 33/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8747\n",
      "Epoch 00033: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.8742 - val_loss: 1.0336\n",
      "Epoch 34/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8526\n",
      "Epoch 00034: val_loss did not improve from 1.00036\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.8522 - val_loss: 1.0333\n",
      "Partial score of fold 1 is: 0.5744939020427505\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 370)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 50, 370)           0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 50, 370, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 2.0477\n",
      "Epoch 00001: val_loss improved from inf to 1.11602, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 414us/sample - loss: 2.0449 - val_loss: 1.1160\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.4272\n",
      "Epoch 00002: val_loss improved from 1.11602 to 1.11299, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 1.4263 - val_loss: 1.1130\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.3126\n",
      "Epoch 00003: val_loss improved from 1.11299 to 1.07416, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 1.3127 - val_loss: 1.0742\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.2444\n",
      "Epoch 00004: val_loss improved from 1.07416 to 1.05969, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 1.2450 - val_loss: 1.0597\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2185\n",
      "Epoch 00005: val_loss improved from 1.05969 to 1.05545, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 1.2179 - val_loss: 1.0554\n",
      "Epoch 6/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.1864\n",
      "Epoch 00006: val_loss improved from 1.05545 to 1.04590, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 1.1861 - val_loss: 1.0459\n",
      "Epoch 7/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1535\n",
      "Epoch 00007: val_loss improved from 1.04590 to 1.03894, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.1524 - val_loss: 1.0389\n",
      "Epoch 8/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1324\n",
      "Epoch 00008: val_loss did not improve from 1.03894\n",
      "14152/14152 [==============================] - 5s 382us/sample - loss: 1.1344 - val_loss: 1.0424\n",
      "Epoch 9/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1178\n",
      "Epoch 00009: val_loss improved from 1.03894 to 1.03576, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 1.1150 - val_loss: 1.0358\n",
      "Epoch 10/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0969\n",
      "Epoch 00010: val_loss did not improve from 1.03576\n",
      "14152/14152 [==============================] - 5s 362us/sample - loss: 1.0968 - val_loss: 1.0527\n",
      "Epoch 11/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0962\n",
      "Epoch 00011: val_loss improved from 1.03576 to 1.02838, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 363us/sample - loss: 1.0951 - val_loss: 1.0284\n",
      "Epoch 12/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0755\n",
      "Epoch 00012: val_loss improved from 1.02838 to 1.02462, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 1.0756 - val_loss: 1.0246\n",
      "Epoch 13/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0707\n",
      "Epoch 00013: val_loss improved from 1.02462 to 1.02035, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 363us/sample - loss: 1.0705 - val_loss: 1.0204\n",
      "Epoch 14/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0625\n",
      "Epoch 00014: val_loss did not improve from 1.02035\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 1.0637 - val_loss: 1.0244\n",
      "Epoch 15/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0374\n",
      "Epoch 00015: val_loss improved from 1.02035 to 1.01151, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 363us/sample - loss: 1.0371 - val_loss: 1.0115\n",
      "Epoch 16/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0304\n",
      "Epoch 00016: val_loss did not improve from 1.01151\n",
      "14152/14152 [==============================] - 5s 364us/sample - loss: 1.0298 - val_loss: 1.0150\n",
      "Epoch 17/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0198\n",
      "Epoch 00017: val_loss did not improve from 1.01151\n",
      "14152/14152 [==============================] - 5s 363us/sample - loss: 1.0208 - val_loss: 1.0247\n",
      "Epoch 18/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0197\n",
      "Epoch 00018: val_loss improved from 1.01151 to 1.01034, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 362us/sample - loss: 1.0200 - val_loss: 1.0103\n",
      "Epoch 19/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00019: val_loss did not improve from 1.01034\n",
      "14152/14152 [==============================] - 5s 382us/sample - loss: 0.9978 - val_loss: 1.0139\n",
      "Epoch 20/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00020: val_loss did not improve from 1.01034\n",
      "14152/14152 [==============================] - 5s 387us/sample - loss: 0.9992 - val_loss: 1.0212\n",
      "Epoch 21/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9868\n",
      "Epoch 00021: val_loss improved from 1.01034 to 1.00735, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 377us/sample - loss: 0.9874 - val_loss: 1.0074\n",
      "Epoch 22/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9853\n",
      "Epoch 00022: val_loss did not improve from 1.00735\n",
      "14152/14152 [==============================] - 5s 371us/sample - loss: 0.9842 - val_loss: 1.0100\n",
      "Epoch 23/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9591\n",
      "Epoch 00023: val_loss did not improve from 1.00735\n",
      "14152/14152 [==============================] - 5s 363us/sample - loss: 0.9600 - val_loss: 1.0207\n",
      "Epoch 24/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9585\n",
      "Epoch 00024: val_loss did not improve from 1.00735\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 0.9587 - val_loss: 1.0176\n",
      "Epoch 25/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9508\n",
      "Epoch 00025: val_loss did not improve from 1.00735\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9507 - val_loss: 1.0081\n",
      "Epoch 26/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9452\n",
      "Epoch 00026: val_loss did not improve from 1.00735\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.9450 - val_loss: 1.0185\n",
      "Epoch 27/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9352\n",
      "Epoch 00027: val_loss improved from 1.00735 to 1.00359, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9351 - val_loss: 1.0036\n",
      "Epoch 28/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9216\n",
      "Epoch 00028: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 362us/sample - loss: 0.9213 - val_loss: 1.0179\n",
      "Epoch 29/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.9188\n",
      "Epoch 00029: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.9189 - val_loss: 1.0114\n",
      "Epoch 30/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00030: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.9137 - val_loss: 1.0265\n",
      "Epoch 31/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8980\n",
      "Epoch 00031: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.8979 - val_loss: 1.0141\n",
      "Epoch 32/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8904\n",
      "Epoch 00032: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.8912 - val_loss: 1.0476\n",
      "Epoch 33/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8983\n",
      "Epoch 00033: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 0.8989 - val_loss: 1.0392\n",
      "Epoch 34/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8795\n",
      "Epoch 00034: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.8780 - val_loss: 1.0231\n",
      "Epoch 35/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8726\n",
      "Epoch 00035: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.8724 - val_loss: 1.0366\n",
      "Epoch 36/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8687\n",
      "Epoch 00036: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.8688 - val_loss: 1.0039\n",
      "Epoch 37/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8655\n",
      "Epoch 00037: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.8653 - val_loss: 1.0242\n",
      "Epoch 38/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8586\n",
      "Epoch 00038: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 0.8574 - val_loss: 1.0172\n",
      "Epoch 39/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8424\n",
      "Epoch 00039: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.8461 - val_loss: 1.0192\n",
      "Epoch 40/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8390\n",
      "Epoch 00040: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.8373 - val_loss: 1.0274\n",
      "Epoch 41/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8311\n",
      "Epoch 00041: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.8326 - val_loss: 1.0237\n",
      "Epoch 42/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8260\n",
      "Epoch 00042: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.8273 - val_loss: 1.0286\n",
      "Epoch 43/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.8193\n",
      "Epoch 00043: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.8192 - val_loss: 1.0225\n",
      "Epoch 44/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8178\n",
      "Epoch 00044: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 0.8179 - val_loss: 1.0353\n",
      "Epoch 45/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8073\n",
      "Epoch 00045: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.8074 - val_loss: 1.0395\n",
      "Epoch 46/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.8073\n",
      "Epoch 00046: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 0.8073 - val_loss: 1.0442\n",
      "Epoch 47/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8014\n",
      "Epoch 00047: val_loss did not improve from 1.00359\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.8007 - val_loss: 1.0542\n",
      "Partial score of fold 2 is: 0.5843352962343606\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 370)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 50, 370)           0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 50, 370, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 7, 18)          45018     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 126)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               12700     \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 63,119\n",
      "Trainable params: 63,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.8824\n",
      "Epoch 00001: val_loss improved from inf to 1.12651, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 6s 447us/sample - loss: 1.8799 - val_loss: 1.1265\n",
      "Epoch 2/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00002: val_loss improved from 1.12651 to 1.08379, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 1.4507 - val_loss: 1.0838\n",
      "Epoch 3/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.3136\n",
      "Epoch 00003: val_loss improved from 1.08379 to 1.06652, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.3140 - val_loss: 1.0665\n",
      "Epoch 4/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.2323\n",
      "Epoch 00004: val_loss improved from 1.06652 to 1.04652, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.2346 - val_loss: 1.0465\n",
      "Epoch 5/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.2018\n",
      "Epoch 00005: val_loss improved from 1.04652 to 1.04009, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.2027 - val_loss: 1.0401\n",
      "Epoch 6/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.1665\n",
      "Epoch 00006: val_loss did not improve from 1.04009\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 1.1648 - val_loss: 1.0421\n",
      "Epoch 7/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.1454\n",
      "Epoch 00007: val_loss improved from 1.04009 to 1.03106, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 1.1451 - val_loss: 1.0311\n",
      "Epoch 8/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1261\n",
      "Epoch 00008: val_loss did not improve from 1.03106\n",
      "14152/14152 [==============================] - 5s 361us/sample - loss: 1.1255 - val_loss: 1.0418\n",
      "Epoch 9/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.1147\n",
      "Epoch 00009: val_loss did not improve from 1.03106\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 1.1133 - val_loss: 1.0345\n",
      "Epoch 10/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.1095\n",
      "Epoch 00010: val_loss improved from 1.03106 to 1.01468, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 1.1091 - val_loss: 1.0147\n",
      "Epoch 11/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 1.0792\n",
      "Epoch 00011: val_loss improved from 1.01468 to 1.01226, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.0797 - val_loss: 1.0123\n",
      "Epoch 12/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0631\n",
      "Epoch 00012: val_loss improved from 1.01226 to 1.00222, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 1.0634 - val_loss: 1.0022\n",
      "Epoch 13/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0659\n",
      "Epoch 00013: val_loss improved from 1.00222 to 1.00027, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.0657 - val_loss: 1.0003\n",
      "Epoch 14/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 1.0577\n",
      "Epoch 00014: val_loss did not improve from 1.00027\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.0571 - val_loss: 1.0139\n",
      "Epoch 15/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 1.0349\n",
      "Epoch 00015: val_loss did not improve from 1.00027\n",
      "14152/14152 [==============================] - 5s 362us/sample - loss: 1.0343 - val_loss: 1.0178\n",
      "Epoch 16/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 1.0317\n",
      "Epoch 00016: val_loss did not improve from 1.00027\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.0315 - val_loss: 1.0095\n",
      "Epoch 17/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 1.0159\n",
      "Epoch 00017: val_loss did not improve from 1.00027\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 1.0176 - val_loss: 1.0089\n",
      "Epoch 18/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0148\n",
      "Epoch 00018: val_loss did not improve from 1.00027\n",
      "14152/14152 [==============================] - 5s 360us/sample - loss: 1.0142 - val_loss: 1.0063\n",
      "Epoch 19/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 1.0015\n",
      "Epoch 00019: val_loss improved from 1.00027 to 0.99765, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 1.0015 - val_loss: 0.9976\n",
      "Epoch 20/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.9940\n",
      "Epoch 00020: val_loss did not improve from 0.99765\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.9942 - val_loss: 1.0000\n",
      "Epoch 21/100\n",
      "14048/14152 [============================>.] - ETA: 0s - loss: 0.9903\n",
      "Epoch 00021: val_loss did not improve from 0.99765\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.9889 - val_loss: 1.0098\n",
      "Epoch 22/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9852\n",
      "Epoch 00022: val_loss did not improve from 0.99765\n",
      "14152/14152 [==============================] - 5s 359us/sample - loss: 0.9859 - val_loss: 1.0255\n",
      "Epoch 23/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9816\n",
      "Epoch 00023: val_loss improved from 0.99765 to 0.99632, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9819 - val_loss: 0.9963\n",
      "Epoch 24/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9538\n",
      "Epoch 00024: val_loss did not improve from 0.99632\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9544 - val_loss: 0.9967\n",
      "Epoch 25/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9523\n",
      "Epoch 00025: val_loss did not improve from 0.99632\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.9526 - val_loss: 0.9972\n",
      "Epoch 26/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.9436\n",
      "Epoch 00026: val_loss did not improve from 0.99632\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9437 - val_loss: 1.0059\n",
      "Epoch 27/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9484\n",
      "Epoch 00027: val_loss improved from 0.99632 to 0.99186, saving model to nn_model.w8\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9486 - val_loss: 0.9919\n",
      "Epoch 28/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9342\n",
      "Epoch 00028: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.9350 - val_loss: 0.9973\n",
      "Epoch 29/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9337\n",
      "Epoch 00029: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 356us/sample - loss: 0.9337 - val_loss: 1.0011\n",
      "Epoch 30/100\n",
      "14112/14152 [============================>.] - ETA: 0s - loss: 0.9155\n",
      "Epoch 00030: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 354us/sample - loss: 0.9156 - val_loss: 1.0210\n",
      "Epoch 31/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.9039\n",
      "Epoch 00031: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 358us/sample - loss: 0.9038 - val_loss: 1.0005\n",
      "Epoch 32/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8984\n",
      "Epoch 00032: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 366us/sample - loss: 0.8982 - val_loss: 1.0112\n",
      "Epoch 33/100\n",
      "14080/14152 [============================>.] - ETA: 0s - loss: 0.8977\n",
      "Epoch 00033: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 386us/sample - loss: 0.8976 - val_loss: 1.0085\n",
      "Epoch 34/100\n",
      "14016/14152 [============================>.] - ETA: 0s - loss: 0.8911\n",
      "Epoch 00034: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 6s 389us/sample - loss: 0.8920 - val_loss: 1.0136\n",
      "Epoch 35/100\n",
      "14144/14152 [============================>.] - ETA: 0s - loss: 0.8881\n",
      "Epoch 00035: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 357us/sample - loss: 0.8883 - val_loss: 1.0195\n",
      "Epoch 36/100\n",
      "13984/14152 [============================>.] - ETA: 0s - loss: 0.8707\n",
      "Epoch 00036: val_loss did not improve from 0.99186\n",
      "14152/14152 [==============================] - 5s 355us/sample - loss: 0.8718 - val_loss: 1.0128\n",
      "Epoch 37/100\n",
      " 2784/14152 [====>.........................] - ETA: 3s - loss: 0.8396"
     ]
    }
   ],
   "source": [
    "cnn_model = Cnn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)\n",
    "nn_model = Nn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "weights = {'lbg': 0.20, 'cat': .20, 'xgb': 0.20, 'nn': 0.20,'cnn':.20}\n",
    "\n",
    "final_pred = (lgb_model.y_pred * weights['lbg']) + (cat_model.y_pred*weights['cat'])+(xgb_model.y_pred * weights['xgb']) + (nn_model.y_pred * weights['nn'])+(cnn_model.y_pred * weights['cnn'])\n",
    "#final_pred = cnn_model.y_pred\n",
    "print(final_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([(round(a, 2), round(b, 2), round(c, 2), round(d, 2)) for a, b, c, d in zip(lgb_model.y_pred, cat_model.y_pred, xgb_model.y_pred, nn_model.y_pred)], columns=['lgb', 'cat', 'xgb', 'nn']).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.280321329387018, 1: 1.7539186712780934, 2: 1.9958694948914841}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    0.500\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.125\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))\n",
    "\n",
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0977e70906ab4ea1ba943349ba9204d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "170f30ec1db64a77ad5ed680a5fc5bd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "34c2b5dcca61478e86482f4c399d58bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4bfcd40d3b224e6bb4c5f89f8e140a3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ddfa121acbe94a4daccffdad75a17ecb",
       "placeholder": "​",
       "style": "IPY_MODEL_fddd870c484f45d783b236744b90b06a",
       "value": " 17000/17000 [08:18&lt;00:00, 34.11it/s]"
      }
     },
     "5389a119ba434c538781a9d9ab00d49e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e21d280d7a174c75a38f4d6a66776cfb",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_170f30ec1db64a77ad5ed680a5fc5bd7",
       "value": 17000
      }
     },
     "55c6accbf7534927ae2c08fa4cc8fb32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f1a25aeaaff347dd85c22ff57dd404e5",
        "IPY_MODEL_a44b833cb39b4511b5533c81c4fa1cc6"
       ],
       "layout": "IPY_MODEL_cfbd953efde2409992ac59d8729c5696"
      }
     },
     "5a05991decf14e16916bef987d2e4f4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a44b833cb39b4511b5533c81c4fa1cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5a05991decf14e16916bef987d2e4f4e",
       "placeholder": "​",
       "style": "IPY_MODEL_e2aaa312746542aaafabf879a12f5b36",
       "value": " 1000/1000 [01:01&lt;00:00, 16.27it/s]"
      }
     },
     "cfbd953efde2409992ac59d8729c5696": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "daa05092a461412f9380cbf0680dd53a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5389a119ba434c538781a9d9ab00d49e",
        "IPY_MODEL_4bfcd40d3b224e6bb4c5f89f8e140a3c"
       ],
       "layout": "IPY_MODEL_0977e70906ab4ea1ba943349ba9204d5"
      }
     },
     "ddfa121acbe94a4daccffdad75a17ecb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e21d280d7a174c75a38f4d6a66776cfb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e2aaa312746542aaafabf879a12f5b36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e8290f83695445ff92418bd4cf8bbbc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1a25aeaaff347dd85c22ff57dd404e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8290f83695445ff92418bd4cf8bbbc9",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_34c2b5dcca61478e86482f4c399d58bb",
       "value": 1000
      }
     },
     "fddd870c484f45d783b236744b90b06a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
